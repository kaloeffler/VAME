{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/katharina/vame_approach/VAME\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vame.analysis.kinutils import KinVideo, create_grid_video\n",
    "import os\n",
    "from datetime import datetime\n",
    "from vame.util.auxiliary import read_config\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "from vame.analysis.visualize import create_aligned_mouse_video, create_pose_snipplet, create_visual_comparison\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s: %(message)s')\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ) Load latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"/home/katharina/vame_approach/themis_tail_belly_align\"\n",
    "\n",
    "anchor_idx = 4000 # select a time point to find similar neighbors to\n",
    "\n",
    "anchor_idx = int(335.6* 120) # standing mouse\n",
    "#anchor_idx = int(373.32 * 120) # walking mouse+ tick at the end\n",
    "\n",
    "SHOW_ALIGNED = True # if True create an aligned video to create snipplets from; otherwise use original video\n",
    "min_dist_nn_factor = 1 # config[\"time_window\"] * min_dist_nn_factor will define the min distance in time points between the anchor and between sampled neighbors\n",
    "align_landmark_idx = [8,16] # landmarks to use for alignment of the videos\n",
    "\n",
    "trained_models = [\n",
    "    (datetime.strptime(element, \"%m-%d-%Y-%H-%M\"), element)\n",
    "    for element in os.listdir(os.path.join(PROJECT_PATH, \"model\"))\n",
    "]\n",
    "# sort by time step\n",
    "trained_models.sort(key=lambda x: x[0])\n",
    "latest_model = trained_models[-1][-1]\n",
    "\n",
    "config_file = os.path.join(PROJECT_PATH, \"model\", latest_model, \"config.yaml\")\n",
    "config = read_config(config_file)\n",
    "# select landmark file\n",
    "landmark_file = config[\"video_sets\"][0]\n",
    "data_path = os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        \"results\",\n",
    "        latest_model,\n",
    "        landmark_file,\n",
    "        config[\"model_name\"],\n",
    "        \"kmeans-\" + str(config[\"n_init_kmeans\"]),\n",
    "    )\n",
    "latent_vectors = np.load(\n",
    "        os.path.join(data_path, \"latent_vector_\" + landmark_file + \".npy\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract landmark data and names\n",
    "landmarks_orig = pd.read_csv(\n",
    "        os.path.join(PROJECT_PATH, \"landmarks\", landmark_file + \".csv\"), header=[0, 1],\n",
    "    )\n",
    "column_names = landmarks_orig.columns\n",
    "landmark_names = [col_name[0] for col_name in column_names if col_name[-1] == \"x\"]\n",
    "\n",
    "landmark_data_file = os.path.join(\n",
    "        PROJECT_PATH, \"data\", landmark_file, landmark_file + \"-PE-seq.npy\"\n",
    "    )\n",
    "landmark_data_aligned = np.load(landmark_data_file).T\n",
    "# reshape to (N_samples, N_landmarks, 2)\n",
    "landmark_data_aligned = landmark_data_aligned.reshape(landmark_data_aligned.shape[0], -1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Sample anchor latent embedding and visualize together with its nearest neighbors vs distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_start = max(0, anchor_idx - int(config[\"time_window\"] * min_dist_nn_factor))\n",
    "window_end = min(len(latent_vectors), anchor_idx + int(config[\"time_window\"] * min_dist_nn_factor))\n",
    "selected_latent_vector = latent_vectors[anchor_idx, :]\n",
    "\n",
    "dist_orig = np.sqrt(np.sum((latent_vectors - selected_latent_vector.reshape(1,-1))**2, axis=1))\n",
    "\n",
    "time_points = np.arange(0, latent_vectors.shape[0])\n",
    "time_points = np.concatenate([time_points[0:window_start], time_points[window_end:-1]])\n",
    "latent_vectors = np.concatenate([latent_vectors[0:window_start], latent_vectors[window_end:-1]])\n",
    "# distances between each latent vector and the selected one excluding the distances of latent vectors corresponding to temporally close frames\n",
    "dist = np.concatenate([dist_orig[0:window_start], dist_orig[window_end:-1]])\n",
    "\n",
    "# select n neighbors, and enshure the neighbors are separated by a min time span\n",
    "selected_neighbor_idx = []\n",
    "while len(selected_neighbor_idx) < 8 and len(dist) > 0:\n",
    "    n_idx = np.argmin(dist)\n",
    "    selected_neighbor_idx.append(time_points[n_idx])\n",
    "    # remove all distances close to the selected anchor\n",
    "    is_far_away = np.abs(time_points - time_points[n_idx]) > int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "    dist = dist[is_far_away]\n",
    "    time_points = time_points[is_far_away]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogramm of distances\n",
    "%matplotlib widget\n",
    "# plot dist showing the neighbors but not the excluded ones?\n",
    "# plotting dist removing only the nearby neighbors\n",
    "bins = 50\n",
    "hist_range = (min(dist_orig), max(dist_orig))\n",
    "dist_wo_nearby = np.concatenate([dist_orig[0:window_start], dist_orig[window_end:-1]])\n",
    "plt.hist(dist_orig,bins=bins, range=hist_range, label=\"all dist\")\n",
    "plt.hist(dist_wo_nearby, bins=bins, range=hist_range, alpha=0.5, label=\"dist w/o nearby\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Visualize anchor vs nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corresponding video\n",
    "video_df = pd.read_csv(os.path.join(PROJECT_PATH, \"video_info.csv\"))\n",
    "video_id = int(re.findall(r\"\\d+\", landmark_file)[0])\n",
    "video_file = os.path.join(\n",
    "            *video_df[video_df[\"video_id\"] == video_id][\n",
    "                [\"vid_folder\", \"vid_file\"]\n",
    "            ].values[0]\n",
    "        )\n",
    "subject, date, camera_pos, video_name = Path(video_file).parts[-4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all selected time points including anchor and its nearest neighbors\n",
    "time_ids = [anchor_idx, *selected_neighbor_idx]\n",
    "# change from frames to seconds by dividing with fps\n",
    "if SHOW_ALIGNED:\n",
    "    video_name, ending = os.path.basename(video_file).split(\".\")\n",
    "    aligned_video_path = os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        \"results\", \"align\", \"a\" + video_name + \".\" + ending)\n",
    "    if not os.path.exists(aligned_video_path):\n",
    "        landmark_file_path = os.path.join(PROJECT_PATH, \"landmarks\", landmark_file+\".csv\")\n",
    "        create_aligned_mouse_video(\n",
    "            video_file,\n",
    "            landmark_file,\n",
    "            align_landmark_idx,\n",
    "            os.path.dirname(aligned_video_path),\n",
    "            crop_size=(300, 300))\n",
    "\n",
    "    selected_video_file = aligned_video_path\n",
    "else:\n",
    "    selected_video_file = video_file\n",
    "#print(selected_video_file)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"]/video.getfps()\n",
    "\n",
    "\n",
    "video_clip_data = [(selected_video_file, t_id/ video.getfps(), (0,0,video.width,video.height)) for t_id in time_ids]\n",
    "print(video_clip_data)\n",
    "grid_video_name = create_grid_video(video_clip_data,video_clip_duration,speed=0.5) # duration is in seconds!!\n",
    "display.Video(grid_video_name, embed=True,html_attributes=\"loop autoplay\", width=600,height=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create full pose video and then sample the snipplets\n",
    "video_name = os.path.basename(video_file)\n",
    "pose_video_file = os.path.join(PROJECT_PATH, \"results\", \"poses_\"+video_name)\n",
    "if not os.path.exists(pose_video_file):\n",
    "    crop_size = 400\n",
    "    # min max normalize the data to a fixed grid shape for visualization\n",
    "    landmark_name = os.path.basename(landmark_file).split(\".\")[0]\n",
    "    # reshape to (N_samples, N_landmarks, 2)\n",
    "    landmark_data_aligned = np.load(\n",
    "        os.path.join(PROJECT_PATH, \"data\", landmark_name, landmark_name + \"-PE-seq.npy\")\n",
    "    ).T\n",
    "    landmark_data_aligned = landmark_data_aligned.reshape(\n",
    "        landmark_data_aligned.shape[0], -1, 2\n",
    "    )\n",
    "    landmark_data_trafo = (\n",
    "        (landmark_data_aligned - landmark_data_aligned.min())\n",
    "        / (landmark_data_aligned.max() - landmark_data_aligned.min())\n",
    "        * (crop_size - 1)\n",
    "    )\n",
    "    column_names = pd.read_csv(landmark_file, header=[0, 1]).columns\n",
    "    landmark_names = [col_name[0] for col_name in column_names if col_name[-1] == \"x\"]\n",
    "    time_ids = np.arange(0, len(landmark_data_trafo))\n",
    "    create_pose_snipplet(\n",
    "        landmark_data_trafo,\n",
    "        landmark_names,\n",
    "        time_ids,\n",
    "        pose_video_file,\n",
    "        crop_size=(crop_size, crop_size),\n",
    "    )\n",
    "pose_video = KinVideo(pose_video_file, view=camera_pos)\n",
    "pose_video.probevid()\n",
    "\n",
    "pose_video_clip_data = [(pose_video_file, t_id/ pose_video.getfps(), (0,0,pose_video.width,pose_video.height)) for t_id in time_ids]\n",
    "pose_grid_video_name = create_grid_video(pose_video_clip_data,video_clip_duration,speed=0.5) # duration is in seconds!!\n",
    "display.Video(pose_grid_video_name, embed=True,html_attributes=\"loop autoplay\", width=900,height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Visualize anchor vs distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Anchor together with distant embeddings\n",
    "# select other embeddings from the 80% distance percentiles\n",
    "dist_percentile = 80\n",
    "\n",
    "dist_thr = np.percentile(dist_orig, dist_percentile)\n",
    "\n",
    "time_idx_other = np.where(dist_orig > dist_thr)[0].reshape(-1)\n",
    "print(time_idx_other)\n",
    "sampled_idx = np.random.choice(time_idx_other, 8, replace=False)\n",
    "# select anchors\n",
    "video_clip_data_distant = [(selected_video_file, t_id/ video.getfps(), (0,0,video.width,video.height)) for t_id in [anchor_idx, *sampled_idx]]\n",
    "print(video_clip_data)\n",
    "grid_video_name_distant = create_grid_video(video_clip_data_distant,video_clip_duration,speed=0.5) # duration is in seconds!!\n",
    "display.Video(grid_video_name_distant, embed=True,html_attributes=\"loop autoplay\", width=600,height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Compare video clips of a bunch of anchors vs neighbors and distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pick_n_anchors = 10 # how many anchor ids to pick randomly\n",
    "\n",
    "latent_vectors = np.load(\n",
    "        os.path.join(data_path, \"latent_vector_\" + landmark_file + \".npy\")\n",
    "    )\n",
    "\n",
    "random_anchor_ids = np.random.choice(np.arange(0, latent_vectors.shape[0]), pick_n_anchors, replace=False)\n",
    "min_frame_distance = int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"]/video.getfps()\n",
    "\n",
    "video_stack = []\n",
    "for a_idx in random_anchor_ids:\n",
    "    video_stack.append(create_visual_comparison(a_idx, latent_vectors, min_frame_distance, selected_video_file,video_clip_duration, upper_dist_percentile=80))\n",
    "\n",
    "# plot next to each other: left side: anchor and its 8 closest neighbors; right side anchor and 8 samples belonging to the \n",
    "# 20% of the most distant latent vectors wrt. the anchor embedding\n",
    "grid = GridspecLayout(pick_n_anchors, 2)\n",
    "# sorted video files\n",
    "for i_row, video_pair in enumerate(video_stack):\n",
    "    for j_vid,video_f in enumerate(video_pair):\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(display.Video(video_f, embed=True, html_attributes=\"loop autoplay\", width=450,height=450))\n",
    "        grid[i_row, j_vid] = out\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T.SNE of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "latent_vectors_all = np.load(\n",
    "        os.path.join(data_path, \"latent_vector_\" + landmark_file + \".npy\")\n",
    "    )\n",
    "tsne = TSNE(perplexity=30).fit_transform(latent_vectors_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "is_neighbor_id = np.isin(np.arange(0,tsne.shape[0]), time_ids[1:])\n",
    "\n",
    "plt.plot (tsne[:, 0], tsne[:, 1], 'r.', label='TSNE')\n",
    "plt.plot (tsne[anchor_id, 0], tsne[anchor_id, 1], 'bo', label='Anchor')\n",
    "plt.plot (tsne[is_neighbor_id, 0], tsne[is_neighbor_id, 1], 'yo', label='Neighbors')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the small paths are the latent vectors corresponding to time series which are very close to the initial time series\n",
    "# since t-SNE tries to preserve the local neighborhood it will prioritize embedding the direct neighbors over embedding\n",
    "# quite similar time series of later frames -> therefore test how the projection looks on a subsampled data set\n",
    "n_fraction = 0.05\n",
    "rand_sub_set = np.random.choice(np.arange(0, latent_vectors_all.shape[0]), int(n_fraction*latent_vectors_all.shape[0]),replace=False)\n",
    "tsne_sub = TSNE(perplexity=30).fit_transform(latent_vectors_all[rand_sub_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "is_neighbor_id = np.isin(rand_sub_set, time_ids[1:])\n",
    "\n",
    "plt.plot (tsne_sub[:, 0], tsne_sub[:, 1], 'r.', label='TSNE sub')\n",
    "if np.isin(anchor_id, rand_sub_set):\n",
    "    plt.plot (tsne_sub[anchor_id, 0], tsne_sub[anchor_id, 1], 'bo', label='Anchor')\n",
    "plt.plot (tsne_sub[is_neighbor_id, 0], tsne_sub[is_neighbor_id, 1], 'yo', label='Neighbors')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mark time windows with little and strong movement (lower and upper N% percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mark time windows with little and strong movement (lower and upper N% percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time points with high motility\n",
    "# idea: calc sum of differences in the tailbase - because when the rat is getting \n",
    "# up the tailbase will also remain constant\n",
    "# use the non aligned data \n",
    "print(landmarks_orig.shape)\n",
    "tailbase_pos = landmarks_orig[[(\"tailbase\",\"x\"), (\"tailbase\",\"y\")]][1:]\n",
    "tailbase_diff = tailbase_pos.diff(periods=1)\n",
    "tailbase_shift = (tailbase_diff[(\"tailbase\", \"x\")]**2 + tailbase_diff[(\"tailbase\", \"y\")]**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc per window the sum of position changes\n",
    "total_vel = [np.sum(tailbase_shift[i:i+config[\"time_window\"]]) for i in rand_sub_set]\n",
    "total_vel = np.array(total_vel)\n",
    "print(total_vel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select slowest 20% as resting; fastest 20% as walking\n",
    "slow_percentile = np.percentile(total_vel, 20)\n",
    "fast_percentile = np.percentile(total_vel, 80)\n",
    "print(f\"Lower percentile {slow_percentile}; upper percentile {fast_percentile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "slow_time_points = total_vel < slow_percentile\n",
    "fast_time_points = total_vel > fast_percentile\n",
    "print(len(fast_time_points))\n",
    "print(tsne_sub.shape)\n",
    "# plot s\n",
    "plt.plot (tsne_sub[:, 0], tsne_sub[:, 1], 'r.', label='t.SNE')\n",
    "plt.plot (tsne_sub [fast_time_points, 0], tsne_sub [fast_time_points, 1], 'b.', label='Fast')\n",
    "plt.plot (tsne_sub [slow_time_points, 0], tsne_sub [slow_time_points, 1], 'y.', label='Slow')\n",
    "plt.legend ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c7bf34dd8152ec82eafc374d8c9e7d2e10dcd4489766d5c4929982680d7481d"
  },
  "kernelspec": {
   "display_name": "Python [conda env:venv_VAME] *",
   "language": "python",
   "name": "conda-env-venv_VAME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
