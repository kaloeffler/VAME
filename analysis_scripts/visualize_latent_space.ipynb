{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/home/katharina/vame_approach/VAME\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vame.analysis.kinutils import KinVideo, create_grid_video\n",
    "import os\n",
    "from datetime import datetime\n",
    "from vame.util.auxiliary import read_config\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from fcmeans import FCM\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from vame.analysis.utils import (\n",
    "    create_aligned_mouse_video,\n",
    "    create_pose_snipplet,\n",
    "    create_visual_comparison,\n",
    "    thin_dataset_iteratively,\n",
    "    find_percentile_threshold,\n",
    ")\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(levelname)s: %(asctime)s: %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ) Load latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"/home/katharina/vame_approach/tb_align_0089\"\n",
    "\n",
    "# select a time point to find similar neighbors to\n",
    "anchor_idx = int(335.6 * 120)  # standing mouse\n",
    "# anchor_idx = int(373.32 * 120) # walking mouse+ tick at the end\n",
    "\n",
    "SHOW_ALIGNED = True  # if True create an aligned video to create snipplets from; otherwise use original video\n",
    "SHOW_POSE_VIDEO = False  # if True show the video of aligned landmark poses\n",
    "min_dist_nn_factor = 1  # config[\"time_window\"] * min_dist_nn_factor will define the min distance in time points between the anchor and between sampled neighbors\n",
    "\n",
    "trained_models = [\n",
    "    (datetime.strptime(element, \"%m-%d-%Y-%H-%M\"), element)\n",
    "    for element in os.listdir(os.path.join(PROJECT_PATH, \"model\"))\n",
    "]\n",
    "# sort by time step\n",
    "trained_models.sort(key=lambda x: x[0])\n",
    "latest_model = trained_models[-1][-1]\n",
    "\n",
    "config_file = os.path.join(PROJECT_PATH, \"model\", latest_model, \"config.yaml\")\n",
    "config = read_config(config_file)\n",
    "# select landmark file\n",
    "landmark_file = config[\"video_sets\"][0]\n",
    "data_path = os.path.join(PROJECT_PATH, \"inference\", \"results\", latest_model)\n",
    "latent_vectors_all = np.load(\n",
    "    os.path.join(data_path, \"latent_vectors_\" + landmark_file + \".npy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Sample anchor latent embedding and visualize together with its nearest neighbors vs distant samples\n",
    "\n",
    "Exploring the latent space visually by sampling an anchor embedding together with vectors that are close to the anchor in the embedding space and visualizing their corresponding time series. To compare the quality of the learned latent space also timeseries corresponding with latent vectors that are far away from the anchor in the latent space are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove latent embeddings that correspond to time series that heavily overlap with the time series of the anchor embedding, to find\n",
    "# similar samples to the anchor that are temporally further appart\n",
    "window_start = max(0, anchor_idx - int(config[\"time_window\"] * min_dist_nn_factor))\n",
    "window_end = min(\n",
    "    len(latent_vectors_all),\n",
    "    anchor_idx + int(config[\"time_window\"] * min_dist_nn_factor),\n",
    ")\n",
    "selected_latent_vector = latent_vectors_all[anchor_idx, :]\n",
    "\n",
    "dist_orig = np.sqrt(\n",
    "    np.sum((latent_vectors_all - selected_latent_vector.reshape(1, -1)) ** 2, axis=1)\n",
    ")\n",
    "\n",
    "time_points = np.arange(0, latent_vectors_all.shape[0])\n",
    "time_points = np.concatenate([time_points[0:window_start], time_points[window_end:-1]])\n",
    "latent_vectors = np.concatenate(\n",
    "    [latent_vectors_all[0:window_start], latent_vectors_all[window_end:-1]]\n",
    ")\n",
    "# distances between each latent vector and the selected one excluding the distances of latent vectors corresponding to temporally close frames\n",
    "dist = np.concatenate([dist_orig[0:window_start], dist_orig[window_end:-1]])\n",
    "\n",
    "# select 8 neighbors, and enshure the neighbors are also separated by a min timespan from each other\n",
    "selected_neighbor_idx = []\n",
    "while len(selected_neighbor_idx) < 8 and len(dist) > 0:\n",
    "    n_idx = np.argmin(dist)\n",
    "    selected_neighbor_idx.append(time_points[n_idx])\n",
    "    # remove all distances close to the selected anchor\n",
    "    is_far_away = np.abs(time_points - time_points[n_idx]) > int(\n",
    "        config[\"time_window\"] * min_dist_nn_factor\n",
    "    )\n",
    "    dist = dist[is_far_away]\n",
    "    time_points = time_points[is_far_away]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogramm of distances\n",
    "%matplotlib widget\n",
    "# plot dist showing the neighbors but not the excluded ones?\n",
    "# plotting dist removing only the nearby neighbors\n",
    "bins = 50\n",
    "hist_range = (min(dist_orig), max(dist_orig))\n",
    "dist_wo_nearby = np.concatenate([dist_orig[0:window_start], dist_orig[window_end:-1]])\n",
    "plt.hist(dist_orig, bins=bins, range=hist_range, label=\"all dist\")\n",
    "plt.hist(\n",
    "    dist_wo_nearby, bins=bins, range=hist_range, alpha=0.5, label=\"dist w/o nearby\"\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: the latent vectors corresponding to time series that are heavily overlapping with the time series corresponding to the anchor embedding are also very close in the latent space. Therefore in a later stage some data dilution is needed to reduce the latent vectors that essentially represent almost the same time series. However, just subsampling the latent vectors such that they would correspond with subsampling the corresponding time series in a fixed grid can potentially destroy the phase information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Visualize anchor vs nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corresponding video\n",
    "video_df = pd.read_csv(os.path.join(PROJECT_PATH, \"video_info.csv\"))\n",
    "video_id = int(re.findall(r\"\\d+\", landmark_file)[0])\n",
    "video_file = os.path.join(\n",
    "    *video_df[video_df[\"video_id\"] == video_id][[\"vid_folder\", \"vid_file\"]].values[0]\n",
    ")\n",
    "subject, date, camera_pos, video_name = Path(video_file).parts[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all selected time points including anchor and its nearest neighbors\n",
    "time_ids = [anchor_idx, *selected_neighbor_idx]\n",
    "# change from frames to seconds by dividing with fps\n",
    "if SHOW_ALIGNED:\n",
    "    video_name, ending = os.path.basename(video_file).split(\".\")\n",
    "    aligned_video_path = os.path.join(\n",
    "        PROJECT_PATH, \"videos\", \"aligned_videos\", \"a\" + video_name + \".\" + ending\n",
    "    )\n",
    "    if not os.path.exists(aligned_video_path):\n",
    "        landmark_file_path = os.path.join(\n",
    "            PROJECT_PATH, \"landmarks\", landmark_file + \".csv\"\n",
    "        )\n",
    "        create_aligned_mouse_video(\n",
    "            video_file,\n",
    "            landmark_file,\n",
    "            os.path.dirname(aligned_video_path),\n",
    "            crop_size=(300, 300),\n",
    "        )\n",
    "\n",
    "    selected_video_file = aligned_video_path\n",
    "else:\n",
    "    selected_video_file = video_file\n",
    "# print(selected_video_file)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"] / video.getfps()\n",
    "\n",
    "\n",
    "video_clip_data = [\n",
    "    (selected_video_file, t_id / video.getfps(), (0, 0, video.width, video.height))\n",
    "    for t_id in time_ids\n",
    "]\n",
    "# print(video_clip_data)\n",
    "grid_video_name = create_grid_video(\n",
    "    video_clip_data, video_clip_duration, speed=0.5\n",
    ")  # duration is in seconds!!\n",
    "dist_matrix = np.round(squareform(pdist(latent_vectors_all[time_ids])), 3)\n",
    "print(f\"Distances between latent embeddings:\\n {dist_matrix}\")\n",
    "display.Video(\n",
    "    grid_video_name, embed=True, html_attributes=\"loop autoplay\", width=600, height=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create full pose video and then sample the snipplets\n",
    "if SHOW_POSE_VIDEO:\n",
    "    video_name = os.path.basename(video_file)\n",
    "    pose_video_file = os.path.join(PROJECT_PATH, \"videos\", \"poses_\" + video_name)\n",
    "    if not os.path.exists(pose_video_file):\n",
    "        crop_size = 400\n",
    "        # min max normalize the data to a fixed grid shape for visualization\n",
    "        landmark_name = os.path.basename(landmark_file).split(\".\")[0]\n",
    "        # reshape to (N_samples, N_landmarks, 2)\n",
    "        landmark_data_aligned = np.load(\n",
    "            os.path.join(\n",
    "                PROJECT_PATH, \"data\", landmark_name, landmark_name + \"-PE-seq.npy\"\n",
    "            )\n",
    "        ).T\n",
    "        landmark_data_aligned = landmark_data_aligned.reshape(\n",
    "            landmark_data_aligned.shape[0], -1, 2\n",
    "        )\n",
    "        landmark_data_trafo = (\n",
    "            (landmark_data_aligned - landmark_data_aligned.min())\n",
    "            / (landmark_data_aligned.max() - landmark_data_aligned.min())\n",
    "            * (crop_size - 1)\n",
    "        )\n",
    "        column_names = pd.read_csv(landmark_file, header=[0, 1]).columns\n",
    "        landmark_names = [\n",
    "            col_name[0] for col_name in column_names if col_name[-1] == \"x\"\n",
    "        ]\n",
    "        time_ids = np.arange(0, len(landmark_data_trafo))\n",
    "        create_pose_snipplet(\n",
    "            landmark_data_trafo,\n",
    "            landmark_names,\n",
    "            time_ids,\n",
    "            pose_video_file,\n",
    "            crop_size=(crop_size, crop_size),\n",
    "        )\n",
    "    pose_video = KinVideo(pose_video_file, view=camera_pos)\n",
    "    pose_video.probevid()\n",
    "\n",
    "    pose_video_clip_data = [\n",
    "        (\n",
    "            pose_video_file,\n",
    "            t_id / pose_video.getfps(),\n",
    "            (0, 0, pose_video.width, pose_video.height),\n",
    "        )\n",
    "        for t_id in time_ids\n",
    "    ]\n",
    "    pose_grid_video_name = create_grid_video(\n",
    "        pose_video_clip_data, video_clip_duration, speed=0.5\n",
    "    )  # duration is in seconds!!\n",
    "    display.Video(\n",
    "        pose_grid_video_name,\n",
    "        embed=True,\n",
    "        html_attributes=\"loop autoplay\",\n",
    "        width=600,\n",
    "        height=600,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Visualize anchor vs distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Anchor together with distant embeddings\n",
    "# select other embeddings from the 80% distance percentiles\n",
    "dist_percentile = 80\n",
    "\n",
    "dist_thr = np.percentile(dist_orig, dist_percentile)\n",
    "\n",
    "time_idx_other = np.where(dist_orig > dist_thr)[0].reshape(-1)\n",
    "sampled_idx = np.random.choice(time_idx_other, 8, replace=False)\n",
    "# select anchors\n",
    "video_clip_data_distant = [\n",
    "    (selected_video_file, t_id / video.getfps(), (0, 0, video.width, video.height))\n",
    "    for t_id in [anchor_idx, *sampled_idx]\n",
    "]\n",
    "# print(video_clip_data)\n",
    "grid_video_name_distant = create_grid_video(\n",
    "    video_clip_data_distant, video_clip_duration, speed=0.5\n",
    ")  # duration is in seconds!!\n",
    "\n",
    "dist_matrix = np.round(\n",
    "    squareform(pdist(latent_vectors_all[[anchor_idx, *sampled_idx]])), 3\n",
    ")\n",
    "print(f\"Distances between latent embeddings:\\n {dist_matrix}\")\n",
    "display.Video(\n",
    "    grid_video_name_distant,\n",
    "    embed=True,\n",
    "    html_attributes=\"loop autoplay\",\n",
    "    width=600,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Compare video clips of a bunch of randomly selected anchors vs neighbors and distant samples\n",
    "Explore the latent space further by sampling several anchors and comparing the anchor and close neighbors vs the anchor and distant samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_n_anchors = 3  # how many anchor ids to pick randomly\n",
    "\n",
    "\n",
    "random_anchor_ids = np.random.choice(\n",
    "    np.arange(0, latent_vectors_all.shape[0]), pick_n_anchors, replace=False\n",
    ")\n",
    "min_frame_distance = int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"] / video.getfps()\n",
    "\n",
    "video_stack = []\n",
    "for a_idx in random_anchor_ids:\n",
    "    video_stack.append(\n",
    "        create_visual_comparison(\n",
    "            a_idx,\n",
    "            latent_vectors_all,\n",
    "            min_frame_distance,\n",
    "            selected_video_file,\n",
    "            video_clip_duration,\n",
    "            upper_dist_percentile=80,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# plot next to each other: left side: anchor and its 8 closest neighbors; right side anchor and 8 samples belonging to the\n",
    "# 20% of the most distant latent vectors wrt. the anchor embedding\n",
    "grid = GridspecLayout(pick_n_anchors, 2)\n",
    "# sorted video files\n",
    "for i_row, video_pair in enumerate(video_stack):\n",
    "    for j_vid, video_f in enumerate(video_pair):\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(\n",
    "                display.Video(\n",
    "                    video_f,\n",
    "                    embed=True,\n",
    "                    html_attributes=\"loop autoplay\",\n",
    "                    width=450,\n",
    "                    height=450,\n",
    "                )\n",
    "            )\n",
    "        grid[i_row, j_vid] = out\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Diluting the latent space by removing samples iteratively\n",
    "\n",
    "To reduce the strong temporal overlap of the latent embeddings while keeping relevant embeddings, anchors will be iteratively selected from the set of latent vectors and their neighborhood is evaluated. All neighboring samples that are very close in latent space as well as in the temporal domain will be removed, while to sampled anchor will be kept. The procedure is repeated until either all remainng latent vectors have been compared or until the remaining size of the data sets falls below a minimum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence of subsampling on neighbor percentile estimation\n",
    "if False:\n",
    "    for i_subsample in [1, 2, 4, 5, 10, 20, 50]:\n",
    "        neighbor_percentile = find_percentile_threshold(\n",
    "            latent_vectors_all[::i_subsample],\n",
    "            config[\"time_window\"],\n",
    "            time_idx=np.arange(0, len(latent_vectors_all))[::i_subsample],\n",
    "            test_fraction=0.01 * i_subsample,\n",
    "        )\n",
    "        print(\n",
    "            f\"subsampling factor: {i_subsample}, neighbor percentile: {neighbor_percentile}\"\n",
    "        )\n",
    "    print(\n",
    "        \"For small subsampling factors (with respect to the time_window) the neighbor percentile stays roughly the same! The fluctuations in the percentile stem from taking a random set of subsamples\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove vectors which are temporally close to the sampled anchor if the belong to its closest N% percentile of embeddings in the latent space\n",
    "sub_sampling_factor = (\n",
    "    config[\"time_window\"] // 10\n",
    ")  # choose a subsampling factor for neighbor percentile estimation to save memory\n",
    "neighbor_percentile = find_percentile_threshold(\n",
    "    latent_vectors_all[::sub_sampling_factor],\n",
    "    config[\"time_window\"],\n",
    "    time_idx=np.arange(0, len(latent_vectors_all))[::sub_sampling_factor],\n",
    "    test_fraction=0.01 * sub_sampling_factor,\n",
    ")\n",
    "print(f\"Selected neigbor percentile {neighbor_percentile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frame_rate = config[\"time_window\"]\n",
    "min_remaining_dataset = 0.001  # minimum fraction of remaining samples  e.g. 0.1 = 10%\n",
    "remaining_embeddings, remaining_time_ids = thin_dataset_iteratively(\n",
    "    latent_vectors_all, min_remaining_dataset, neighbor_percentile, min_frame_rate\n",
    ")\n",
    "percentile_data = np.round(len(remaining_embeddings) / len(latent_vectors_all) * 100, 2)\n",
    "print(\n",
    "    f\"{len(remaining_embeddings)} remaining samples from orignially {len(latent_vectors_all)}. So just {percentile_data}% of the original dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TSNE and a UMAP projection of the diluted latent space\n",
    "\n",
    "tsne_thinned = TSNE(perplexity=30).fit_transform(remaining_embeddings)\n",
    "umap_func = umap.UMAP(\n",
    "    densmap=True,\n",
    "    n_components=2,\n",
    "    min_dist=0.0001,\n",
    "    n_neighbors=30,\n",
    "    random_state=config[\"random_state\"],\n",
    ")\n",
    "umap_vectors = umap_func.fit_transform(remaining_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Sample and visualize anchors and neighbors / distant sampples from the thinned dataset\n",
    "\n",
    "Explore the diluted latent space by again picking anchors and its closest neighbors and visualizing their corresponding time series as well as sampling distant samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_n_anchors = 5  # how many anchor ids to pick randomly\n",
    "\n",
    "random_anchor_ids = np.random.choice(\n",
    "    np.arange(0, remaining_embeddings.shape[0]), pick_n_anchors, replace=False\n",
    ")\n",
    "min_frame_distance = int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"] / video.getfps()\n",
    "\n",
    "video_stack = []\n",
    "sampled_idx_stack = []\n",
    "for a_idx in random_anchor_ids:\n",
    "    output = create_visual_comparison(\n",
    "        a_idx,\n",
    "        remaining_embeddings,\n",
    "        min_frame_distance,\n",
    "        selected_video_file,\n",
    "        video_clip_duration,\n",
    "        upper_dist_percentile=80,\n",
    "        time_idx=remaining_time_ids,\n",
    "        return_sampled_idx=True,\n",
    "    )\n",
    "    video_close, video_distant, samples_close, samples_distant = output\n",
    "    video_stack.append((video_close, video_distant))\n",
    "    sampled_idx_stack.append((samples_close, samples_distant))\n",
    "\n",
    "# plot next to each other: left side: anchor and its 8 closest neighbors; right side anchor and 8 samples belonging to the\n",
    "# 20% of the most distant latent vectors wrt. the anchor embedding\n",
    "grid = GridspecLayout(pick_n_anchors * 2, 2)\n",
    "# sorted video files\n",
    "for i_row, video_pair in enumerate(video_stack):\n",
    "    for j_vid, video_f in enumerate(video_pair):\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(\n",
    "                display.Video(\n",
    "                    video_f,\n",
    "                    embed=True,\n",
    "                    html_attributes=\"loop autoplay\",\n",
    "                    width=450,\n",
    "                    height=450,\n",
    "                )\n",
    "            )\n",
    "        grid[i_row * 2, j_vid] = out\n",
    "\n",
    "        dist_matrix = np.round(\n",
    "            squareform(pdist(latent_vectors_all[sampled_idx_stack[i_row][j_vid]])), 2\n",
    "        )\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(display.Pretty(f\"Distances: \\n {dist_matrix}\"))\n",
    "        grid[i_row * 2 + 1, j_vid] = out\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Visualize the sampled anchors with their neighbors and distant samples in a t-SNE and UMAP\n",
    "\n",
    "Idea: check how close samples which are neighbors in the latent space will be projected in t-SNE / UMAP.\n",
    "To use the t-SNE / UMAP projections for clustering the anchor and its close neighbors should also be close in the t-SNE/ UMPA projection and samples which are distant to the selected anchor should also be distant to the anchor in the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the sampled points in the TSNE plot\n",
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots(len(sampled_idx_stack), 2)\n",
    "fig.set_size_inches(4 * 2, len(sampled_idx_stack) * 4)\n",
    "for i_row, (s_close, s_distant) in enumerate(sampled_idx_stack):\n",
    "    anchor_idx = s_close[0]\n",
    "    ax[i_row, 0].plot(tsne_thinned[:, 0], tsne_thinned[:, 1], \"k.\", label=\"TSNE\")\n",
    "    ax[i_row, 0].plot(\n",
    "        tsne_thinned[np.isin(remaining_time_ids, s_close[1:]), 0],\n",
    "        tsne_thinned[np.isin(remaining_time_ids, s_close[1:]), 1],\n",
    "        \"go\",\n",
    "        label=\"Close Neighbors\",\n",
    "    )\n",
    "    ax[i_row, 0].plot(\n",
    "        tsne_thinned[np.isin(remaining_time_ids, s_distant[1:]), 0],\n",
    "        tsne_thinned[np.isin(remaining_time_ids, s_distant[1:]), 1],\n",
    "        \"ro\",\n",
    "        label=\"Distant Samples\",\n",
    "    )\n",
    "    ax[i_row, 0].plot(\n",
    "        tsne_thinned[remaining_time_ids == anchor_idx, 0],\n",
    "        tsne_thinned[remaining_time_ids == anchor_idx, 1],\n",
    "        \"yo\",\n",
    "        label=\"Anchor\",\n",
    "    )\n",
    "\n",
    "    ax[i_row, 1].plot(umap_vectors[:, 0], umap_vectors[:, 1], \"k.\", label=\"UMAP\")\n",
    "    ax[i_row, 1].plot(\n",
    "        umap_vectors[np.isin(remaining_time_ids, s_close[1:]), 0],\n",
    "        umap_vectors[np.isin(remaining_time_ids, s_close[1:]), 1],\n",
    "        \"go\",\n",
    "        label=\"Close Neighbors\",\n",
    "    )\n",
    "    ax[i_row, 1].plot(\n",
    "        umap_vectors[np.isin(remaining_time_ids, s_distant[1:]), 0],\n",
    "        umap_vectors[np.isin(remaining_time_ids, s_distant[1:]), 1],\n",
    "        \"ro\",\n",
    "        label=\"Distant Samples\",\n",
    "    )\n",
    "    ax[i_row, 1].plot(\n",
    "        umap_vectors[remaining_time_ids == anchor_idx, 0],\n",
    "        umap_vectors[remaining_time_ids == anchor_idx, 1],\n",
    "        \"yo\",\n",
    "        label=\"Anchor\",\n",
    "    )\n",
    "\n",
    "    ax[i_row, 0].legend()\n",
    "    ax[i_row, 1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: after running the code multiple times with different anchors, the close neighbors in the latent space tend to be also closer than the distant neighbors in the t-SNE/UMAP projection. However, quite often the anchor and its close neighbors are not concentrated at one point in the projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)  Cluster the diluted dataset\n",
    "Test different clustering approaches and visualize samples from the clusters for visual inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Fuzzy C-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_fcm = 10\n",
    "fcm = FCM(n_clusters=n_clusters_fcm, m=1.1)\n",
    "\n",
    "fcm.fit(remaining_embeddings)\n",
    "\n",
    "# output\n",
    "fcm_centers = fcm.centers\n",
    "# output is [N,K]: N number of latent embeddings and K the number of clusters; for where each entry is a membership score between 0...1\n",
    "fcm_labels_soft = fcm.soft_predict(remaining_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the fuzzy clusters that where found in the high dim latent space by plotting for each sample its membership encoded as alpha (transparency) in the UMAP projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "n_cols = int(n_clusters_fcm ** 0.5)\n",
    "n_rows = int(np.ceil(n_clusters_fcm / n_cols))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(3 * n_cols, 3 * n_rows)\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_fcm)\n",
    "for i_cluster in range(n_clusters_fcm):\n",
    "    i_col = i_cluster % n_cols\n",
    "    i_row = i_cluster // n_cols\n",
    "    ax[i_row][i_col].scatter(\n",
    "        umap_vectors[:, 0], umap_vectors[:, 1], color=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    ax[i_row][i_col].scatter(\n",
    "        umap_vectors[:, 0],\n",
    "        umap_vectors[:, 1],\n",
    "        color=cmap(i_cluster),\n",
    "        alpha=list(fcm_labels_soft[:, i_cluster]),\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: the found clusters in the high dim space are also close in the UMAP projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many samples have a cluster membership score of at least thr?\n",
    "The fuzzy c-means algorithm will predict for each sample its membership to all of the K clusters. The membership is a score between 0 and 1 which is NOT a probability therefore the membership scores of a sample can not sum up to one.\n",
    "To investigate how high the membership scores are plot a histogramm based on the largest membership score of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogramm of the largest cluster assignment score per sample\n",
    "%matplotlib widget\n",
    "cluster_membership_thr = 0.6\n",
    "\n",
    "fcm_max_val = np.max(fcm_labels_soft, axis=1)\n",
    "plt.hist(fcm_max_val, bins=20, range=(0, 1), density=True, cumulative=-1)\n",
    "plt.xlabel(\"Maximum Membership Score\")\n",
    "thr_fraction = round(\n",
    "    sum(fcm_max_val > cluster_membership_thr) / len(fcm_max_val) * 100, 2\n",
    ")\n",
    "print(\n",
    "    f\"{thr_fraction}% of the samples have a largest cluster membership score above {cluster_membership_thr}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate how the found clusters of the fuzzy c-means clustering a projected in the t-SNE/UMAP embedding. Therefore assign each sample to the cluster with its highest membership score and mark samples that are below a minim membership score as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict label from soft labels and set for samples with max value < cluster_membership_thr to -1\n",
    "fcm_labels = np.ones(fcm_labels_soft.shape[0]) * -1\n",
    "fcm_labels[np.max(fcm_labels_soft, axis=1) > cluster_membership_thr] = np.argmax(\n",
    "    fcm_labels_soft, axis=1\n",
    ")[np.max(fcm_labels_soft, axis=1) > cluster_membership_thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay found clusters with t-SNE / UMAP Projection\n",
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(4.5 * 2, 4.5)\n",
    "\n",
    "ax[0].plot(tsne_thinned[:, 0], tsne_thinned[:, 1], \"k.\", label=\"TSNE\")\n",
    "ax[1].plot(umap_vectors[:, 0], umap_vectors[:, 1], \"k.\", label=f\"UMAP\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_fcm)\n",
    "for i_cluster in range(n_clusters_fcm):\n",
    "    anchor_idx = s_close[0]\n",
    "    ax[0].plot(\n",
    "        tsne_thinned[fcm_labels == i_cluster, 0],\n",
    "        tsne_thinned[fcm_labels == i_cluster, 1],\n",
    "        \".\",\n",
    "        color=cmap(i_cluster),\n",
    "        label=f\"Cluster: {i_cluster}\",\n",
    "    )\n",
    "    ax[1].plot(\n",
    "        umap_vectors[fcm_labels == i_cluster, 0],\n",
    "        umap_vectors[fcm_labels == i_cluster, 1],\n",
    "        \".\",\n",
    "        color=cmap(i_cluster),\n",
    "        label=f\"Cluster: {i_cluster}\",\n",
    "    )\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Some clusters found by the fuzzy-c-means clustering that was performed on the highdim latent space tend to also to be placed close in the t-SNE/UMAP projections.\n",
    "\n",
    "### Sample from the found clusters and show the corresponding time series\n",
    "Examine the quality found clusters of the diluted latent space by sampling latent vectors assigned to the same cluster and plotting their corresponding time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a cluster, sample from it and visualize\n",
    "# select only the points with a high assignment score to sample from\n",
    "cluster_id = 2\n",
    "\n",
    "\n",
    "# all selected time points including anchor and its nearest neighbors\n",
    "time_ids_cluster = np.random.choice(\n",
    "    remaining_time_ids[fcm_labels == cluster_id],\n",
    "    min(np.sum(fcm_labels == cluster_id), 16),\n",
    "    replace=False,\n",
    ")\n",
    "\n",
    "video_clip_data_cluster = [\n",
    "    (selected_video_file, t_id / video.getfps(), (0, 0, video.width, video.height))\n",
    "    for t_id in time_ids_cluster\n",
    "]\n",
    "grid_video_cluster = create_grid_video(\n",
    "    video_clip_data_cluster, video_clip_duration, speed=0.5, nrows=4, ncols=4\n",
    ")  # duration is in seconds!!\n",
    "\n",
    "# print Euclidean distances between the samples from the cluster\n",
    "dist_matrix_cluster = np.round(\n",
    "    squareform(pdist(latent_vectors_all[time_ids_cluster])), 2\n",
    ")\n",
    "#print(f\"Distances latent embeddings:\\n {dist_matrix_cluster}\")\n",
    "\n",
    "display.Video(\n",
    "    grid_video_cluster,\n",
    "    embed=True,\n",
    "    html_attributes=\"loop autoplay\",\n",
    "    width=600,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2) FlAME Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/katharina/flame-clustering/\")\n",
    "from pyflame import flame_clustering\n",
    "\n",
    "flame_labels_soft = flame_clustering(\n",
    "    remaining_embeddings,\n",
    "    fuzzy_clusters=True,\n",
    "    knn=30,\n",
    "    epsilon=1e-1,\n",
    "    min_membership_thr=0.3,\n",
    ")\n",
    "n_clusters_flame = flame_labels_soft.shape[-1]\n",
    "flame_labels = np.argmax(flame_labels_soft, axis=1)\n",
    "print(f\"Found {np.max(flame_labels)+1} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.hist(\n",
    "    np.max(flame_labels_soft, axis=1),\n",
    "    bins=20,\n",
    "    range=(0, 1),\n",
    "    density=True,\n",
    "    cumulative=-1,\n",
    ")\n",
    "plt.xlabel(\"Maximum Membership Score\")\n",
    "thr_fraction = round(\n",
    "    sum(np.max(flame_labels_soft, axis=1) > 0.5) / flame_labels_soft.shape[0] * 100, 2,\n",
    ")\n",
    "print(\n",
    "    f\"{thr_fraction}% of the samples have a largest cluster membership score above {cluster_membership_thr}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_membership_thr_flame = 0.5\n",
    "flame_labels[np.max(flame_labels_soft, axis=1) < cluster_membership_thr_flame] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "n_cols = int(n_clusters_flame ** 0.5)\n",
    "n_rows = int(np.ceil(n_clusters_flame / n_cols))\n",
    "fig, ax = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(3 * n_cols, 3 * n_rows)\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_flame)\n",
    "for i_cluster in range(n_clusters_flame):\n",
    "    i_col = i_cluster % n_cols\n",
    "    i_row = i_cluster // n_cols\n",
    "    ax[i_row][i_col].scatter(\n",
    "        umap_vectors[:, 0], umap_vectors[:, 1], color=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    ax[i_row][i_col].scatter(\n",
    "        umap_vectors[:, 0],\n",
    "        umap_vectors[:, 1],\n",
    "        color=cmap(i_cluster),\n",
    "        alpha=list(flame_labels_soft[:, i_cluster]),\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(4.5 * 2, 4.5)\n",
    "\n",
    "ax[0].plot(tsne_thinned[:, 0], tsne_thinned[:, 1], \"k.\", label=\"TSNE\")\n",
    "ax[1].plot(umap_vectors[:, 0], umap_vectors[:, 1], \"k.\", label=f\"UMAP\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_flame)\n",
    "for i_cluster in range(n_clusters_flame):\n",
    "    ax[0].plot(\n",
    "        tsne_thinned[flame_labels == i_cluster, 0],\n",
    "        tsne_thinned[flame_labels == i_cluster, 1],\n",
    "        \".\",\n",
    "        color=cmap(i_cluster),\n",
    "        label=f\"Cluster: {i_cluster}\",\n",
    "    )\n",
    "    ax[1].plot(\n",
    "        umap_vectors[flame_labels == i_cluster, 0],\n",
    "        umap_vectors[flame_labels == i_cluster, 1],\n",
    "        \".\",\n",
    "        color=cmap(i_cluster),\n",
    "        label=f\"Cluster: {i_cluster}\",\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from the found clusters and show the corresponding time series\n",
    "Examine the quality found clusters of the diluted latent space by sampling latent vectors assigned to the same cluster and plotting their corresponding time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a cluster, sample from it and visualize\n",
    "# select only the points with a high assignment score to sample from\n",
    "cluster_id = 15\n",
    "\n",
    "# all selected time points including anchor and its nearest neighbors\n",
    "time_ids_cluster = np.random.choice(\n",
    "    remaining_time_ids[flame_labels == cluster_id],\n",
    "    min(np.sum(flame_labels == cluster_id), 16),\n",
    "    replace=False,\n",
    ")\n",
    "\n",
    "video_clip_data_cluster = [\n",
    "    (selected_video_file, t_id / video.getfps(), (0, 0, video.width, video.height))\n",
    "    for t_id in time_ids_cluster\n",
    "]\n",
    "grid_video_cluster = create_grid_video(\n",
    "    video_clip_data_cluster, video_clip_duration, speed=0.5, nrows=4, ncols=4\n",
    ")  # duration is in seconds!!\n",
    "\n",
    "# print Euclidean distances between the samples from the cluster\n",
    "dist_matrix_cluster = np.round(\n",
    "    squareform(pdist(latent_vectors_all[time_ids_cluster])), 2\n",
    ")\n",
    "# print(f\"Distances latent embeddings:\\n {dist_matrix_cluster}\")\n",
    "\n",
    "display.Video(\n",
    "    grid_video_cluster,\n",
    "    embed=True,\n",
    "    html_attributes=\"loop autoplay\",\n",
    "    width=600,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do PCA and show highdim clusters / show\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_vectors = pca.fit_transform(remaining_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig, ax = plt.subplots(3, 2, sharex=True, sharey=True)\n",
    "fig.set_size_inches(4.5 * 2, 4.5 * 3)\n",
    "\n",
    "for i_dim in range(3):\n",
    "    ax[i_dim][0].plot(\n",
    "        pca_vectors[:, i_dim], pca_vectors[:, (i_dim + 1) % 3], \"k.\", label=\"PCA\"\n",
    "    )\n",
    "    ax[i_dim][1].plot(\n",
    "        pca_vectors[:, i_dim], pca_vectors[:, (i_dim + 1) % 3], \"k.\", label=\"PCA\"\n",
    "    )\n",
    "\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_fcm)\n",
    "ax[0][0].set_title(\"FCM Labels\")\n",
    "for i_cluster in range(n_clusters_fcm):\n",
    "    for i_dim in range(3):\n",
    "        ax[i_dim][0].plot(\n",
    "            pca_vectors[fcm_labels == i_cluster, i_dim],\n",
    "            pca_vectors[fcm_labels == i_cluster, (i_dim + 1) % 3],\n",
    "            \".\",\n",
    "            color=cmap(i_cluster),\n",
    "            label=f\"Cluster: {i_cluster}\",\n",
    "        )\n",
    "\n",
    "\n",
    "ax[0][1].set_title(\"FLAME Labels\")\n",
    "for i_cluster in range(n_clusters_flame):\n",
    "    for i_dim in range(3):\n",
    "        ax[i_dim][1].plot(\n",
    "            pca_vectors[flame_labels == i_cluster, i_dim],\n",
    "            pca_vectors[flame_labels == i_cluster, (i_dim + 1) % 3],\n",
    "            \".\",\n",
    "            color=cmap(i_cluster),\n",
    "            label=f\"Cluster: {i_cluster}\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from numpy import linalg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "\n",
    "# set colour map so each ellipsoid as a unique colour\n",
    "norm = colors.Normalize(vmin=0, vmax=n_clusters_fcm)\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_fcm)\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "cluster_means_fcm = [\n",
    "    np.mean(pca_vectors[fcm_labels == i_cluster], axis=0)\n",
    "    for i_cluster in range(n_clusters_fcm)\n",
    "]\n",
    "cluster_cov_fcm = [\n",
    "    np.cov(pca_vectors[fcm_labels == i_cluster].T)\n",
    "    for i_cluster in range(n_clusters_fcm)\n",
    "]\n",
    "\n",
    "for i_cluster in range(n_clusters_fcm):\n",
    "    # your ellispsoid and center in matrix form\n",
    "\n",
    "    center = cluster_means_fcm[i_cluster]\n",
    "    A = cluster_cov_fcm[i_cluster]\n",
    "    # calc eigenvalues (the srt is the radius) and the eigenvectors (rotation) of the ellipsoid!\n",
    "    eigen_vals, eigen_vec = linalg.eig(A)\n",
    "    radii = np.sqrt(eigen_vals)\n",
    "\n",
    "    # calculate cartesian coordinates for the ellipsoid surface\n",
    "    u = np.linspace(0.0, 2.0 * np.pi, 60)\n",
    "    v = np.linspace(0.0, np.pi, 60)\n",
    "    x = radii[0] * np.outer(np.cos(u), np.sin(v))\n",
    "    y = radii[1] * np.outer(np.sin(u), np.sin(v))\n",
    "    z = radii[2] * np.outer(np.ones_like(u), np.cos(v))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            [x[i, j], y[i, j], z[i, j]] = (\n",
    "                np.dot(eigen_vec, [x[i, j], y[i, j], z[i, j]]) + center\n",
    "            )\n",
    "    ax.plot_surface(\n",
    "        x,\n",
    "        y,\n",
    "        z,\n",
    "        rstride=3,\n",
    "        cstride=3,\n",
    "        color=m.to_rgba(i_cluster),\n",
    "        linewidth=0.1,\n",
    "        alpha=0.3,\n",
    "        shade=True,\n",
    "    )\n",
    "    ax.plot(\n",
    "        pca_vectors[fcm_labels == i_cluster, 0],\n",
    "        pca_vectors[fcm_labels == i_cluster, 1],\n",
    "        pca_vectors[fcm_labels == i_cluster, 2],\n",
    "        \".\",\n",
    "        color=m.to_rgba(i_cluster),\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "min_val = np.amin(pca_vectors)  # lowest number in the array\n",
    "max_val = np.amax(pca_vectors)  # highest number in the array\n",
    "\n",
    "ax.set_xlim3d(min_val, max_val)\n",
    "ax.set_ylim3d(min_val, max_val)\n",
    "ax.set_zlim3d(min_val, max_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Dimension Reduction\n",
    "check how much of the variance is explained using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dim_red = PCA(n_components=20)\n",
    "pca_dim_red.fit(remaining_embeddings)\n",
    "\n",
    "print(\n",
    "    \"Explained variance cumulated over the dimensions:\",\n",
    "    pca_dim_red.explained_variance_ratio_.cumsum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c7bf34dd8152ec82eafc374d8c9e7d2e10dcd4489766d5c4929982680d7481d"
  },
  "kernelspec": {
   "display_name": "Python [conda env:venv_VAME] *",
   "language": "python",
   "name": "conda-env-venv_VAME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
