{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/katharina/vame_approach/VAME\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vame.analysis.kinutils import KinVideo, create_grid_video\n",
    "import os\n",
    "from datetime import datetime\n",
    "from vame.util.auxiliary import read_config\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from fcmeans import FCM\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from vame.analysis.visualize import create_aligned_mouse_video, create_pose_snipplet, create_visual_comparison,thin_dataset_iteratively\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s: %(message)s')\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ) Load latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"/home/katharina/vame_approach/themis_tail_belly_align\"\n",
    "\n",
    "anchor_idx = 4000 # select a time point to find similar neighbors to\n",
    "\n",
    "anchor_idx = int(335.6* 120) # standing mouse\n",
    "#anchor_idx = int(373.32 * 120) # walking mouse+ tick at the end\n",
    "\n",
    "SHOW_ALIGNED = True # if True create an aligned video to create snipplets from; otherwise use original video\n",
    "min_dist_nn_factor = 1 # config[\"time_window\"] * min_dist_nn_factor will define the min distance in time points between the anchor and between sampled neighbors\n",
    "align_landmark_idx = [8,16] # landmarks to use for alignment of the videos\n",
    "\n",
    "trained_models = [\n",
    "    (datetime.strptime(element, \"%m-%d-%Y-%H-%M\"), element)\n",
    "    for element in os.listdir(os.path.join(PROJECT_PATH, \"model\"))\n",
    "]\n",
    "# sort by time step\n",
    "trained_models.sort(key=lambda x: x[0])\n",
    "latest_model = trained_models[-1][-1]\n",
    "\n",
    "config_file = os.path.join(PROJECT_PATH, \"model\", latest_model, \"config.yaml\")\n",
    "config = read_config(config_file)\n",
    "# select landmark file\n",
    "landmark_file = config[\"video_sets\"][0]\n",
    "data_path = os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        \"results\",\n",
    "        latest_model,\n",
    "        landmark_file,\n",
    "        config[\"model_name\"],\n",
    "        \"kmeans-\" + str(config[\"n_init_kmeans\"]),\n",
    "    )\n",
    "latent_vectors_all = np.load(\n",
    "        os.path.join(data_path, \"latent_vector_\" + landmark_file + \".npy\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract landmark data and names\n",
    "landmarks_orig = pd.read_csv(\n",
    "        os.path.join(PROJECT_PATH, \"landmarks\", landmark_file + \".csv\"), header=[0, 1],\n",
    "    )\n",
    "column_names = landmarks_orig.columns\n",
    "landmark_names = [col_name[0] for col_name in column_names if col_name[-1] == \"x\"]\n",
    "\n",
    "landmark_data_file = os.path.join(\n",
    "        PROJECT_PATH, \"data\", landmark_file, landmark_file + \"-PE-seq.npy\"\n",
    "    )\n",
    "landmark_data_aligned = np.load(landmark_data_file).T\n",
    "# reshape to (N_samples, N_landmarks, 2)\n",
    "landmark_data_aligned = landmark_data_aligned.reshape(landmark_data_aligned.shape[0], -1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Sample anchor latent embedding and visualize together with its nearest neighbors vs distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_start = max(0, anchor_idx - int(config[\"time_window\"] * min_dist_nn_factor))\n",
    "window_end = min(len(latent_vectors_all), anchor_idx + int(config[\"time_window\"] * min_dist_nn_factor))\n",
    "selected_latent_vector = latent_vectors_all[anchor_idx, :]\n",
    "\n",
    "dist_orig = np.sqrt(np.sum((latent_vectors_all - selected_latent_vector.reshape(1,-1))**2, axis=1))\n",
    "\n",
    "time_points = np.arange(0, latent_vectors_all.shape[0])\n",
    "time_points = np.concatenate([time_points[0:window_start], time_points[window_end:-1]])\n",
    "latent_vectors = np.concatenate([latent_vectors_all[0:window_start], latent_vectors_all[window_end:-1]])\n",
    "# distances between each latent vector and the selected one excluding the distances of latent vectors corresponding to temporally close frames\n",
    "dist = np.concatenate([dist_orig[0:window_start], dist_orig[window_end:-1]])\n",
    "\n",
    "# select n neighbors, and enshure the neighbors are separated by a min timespan\n",
    "selected_neighbor_idx = []\n",
    "while len(selected_neighbor_idx) < 8 and len(dist) > 0:\n",
    "    n_idx = np.argmin(dist)\n",
    "    selected_neighbor_idx.append(time_points[n_idx])\n",
    "    # remove all distances close to the selected anchor\n",
    "    is_far_away = np.abs(time_points - time_points[n_idx]) > int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "    dist = dist[is_far_away]\n",
    "    time_points = time_points[is_far_away]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogramm of distances\n",
    "%matplotlib widget\n",
    "# plot dist showing the neighbors but not the excluded ones?\n",
    "# plotting dist removing only the nearby neighbors\n",
    "bins = 50\n",
    "hist_range = (min(dist_orig), max(dist_orig))\n",
    "dist_wo_nearby = np.concatenate([dist_orig[0:window_start], dist_orig[window_end:-1]])\n",
    "plt.hist(dist_orig,bins=bins, range=hist_range, label=\"all dist\")\n",
    "plt.hist(dist_wo_nearby, bins=bins, range=hist_range, alpha=0.5, label=\"dist w/o nearby\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Visualize anchor vs nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corresponding video\n",
    "video_df = pd.read_csv(os.path.join(PROJECT_PATH, \"video_info.csv\"))\n",
    "video_id = int(re.findall(r\"\\d+\", landmark_file)[0])\n",
    "video_file = os.path.join(\n",
    "            *video_df[video_df[\"video_id\"] == video_id][\n",
    "                [\"vid_folder\", \"vid_file\"]\n",
    "            ].values[0]\n",
    "        )\n",
    "subject, date, camera_pos, video_name = Path(video_file).parts[-4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all selected time points including anchor and its nearest neighbors\n",
    "time_ids = [anchor_idx, *selected_neighbor_idx]\n",
    "# change from frames to seconds by dividing with fps\n",
    "if SHOW_ALIGNED:\n",
    "    video_name, ending = os.path.basename(video_file).split(\".\")\n",
    "    aligned_video_path = os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        \"results\", \"align\", \"a\" + video_name + \".\" + ending)\n",
    "    if not os.path.exists(aligned_video_path):\n",
    "        landmark_file_path = os.path.join(PROJECT_PATH, \"landmarks\", landmark_file+\".csv\")\n",
    "        create_aligned_mouse_video(\n",
    "            video_file,\n",
    "            landmark_file,\n",
    "            align_landmark_idx,\n",
    "            os.path.dirname(aligned_video_path),\n",
    "            crop_size=(300, 300))\n",
    "\n",
    "    selected_video_file = aligned_video_path\n",
    "else:\n",
    "    selected_video_file = video_file\n",
    "#print(selected_video_file)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"]/video.getfps()\n",
    "\n",
    "\n",
    "video_clip_data = [(selected_video_file, t_id/ video.getfps(), (0,0,video.width,video.height)) for t_id in time_ids]\n",
    "#print(video_clip_data)\n",
    "grid_video_name = create_grid_video(video_clip_data,video_clip_duration,speed=0.5) # duration is in seconds!!\n",
    "dist_matrix = np.round(squareform(pdist(latent_vectors_all[time_ids])), 3)\n",
    "print(f\"Distances:\\n {dist_matrix}\")\n",
    "display.Video(grid_video_name, embed=True,html_attributes=\"loop autoplay\", width=600,height=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create full pose video and then sample the snipplets\n",
    "video_name = os.path.basename(video_file)\n",
    "pose_video_file = os.path.join(PROJECT_PATH, \"results\", \"poses_\"+video_name)\n",
    "if not os.path.exists(pose_video_file):\n",
    "    crop_size = 400\n",
    "    # min max normalize the data to a fixed grid shape for visualization\n",
    "    landmark_name = os.path.basename(landmark_file).split(\".\")[0]\n",
    "    # reshape to (N_samples, N_landmarks, 2)\n",
    "    landmark_data_aligned = np.load(\n",
    "        os.path.join(PROJECT_PATH, \"data\", landmark_name, landmark_name + \"-PE-seq.npy\")\n",
    "    ).T\n",
    "    landmark_data_aligned = landmark_data_aligned.reshape(\n",
    "        landmark_data_aligned.shape[0], -1, 2\n",
    "    )\n",
    "    landmark_data_trafo = (\n",
    "        (landmark_data_aligned - landmark_data_aligned.min())\n",
    "        / (landmark_data_aligned.max() - landmark_data_aligned.min())\n",
    "        * (crop_size - 1)\n",
    "    )\n",
    "    column_names = pd.read_csv(landmark_file, header=[0, 1]).columns\n",
    "    landmark_names = [col_name[0] for col_name in column_names if col_name[-1] == \"x\"]\n",
    "    time_ids = np.arange(0, len(landmark_data_trafo))\n",
    "    create_pose_snipplet(\n",
    "        landmark_data_trafo,\n",
    "        landmark_names,\n",
    "        time_ids,\n",
    "        pose_video_file,\n",
    "        crop_size=(crop_size, crop_size),\n",
    "    )\n",
    "pose_video = KinVideo(pose_video_file, view=camera_pos)\n",
    "pose_video.probevid()\n",
    "\n",
    "pose_video_clip_data = [(pose_video_file, t_id/ pose_video.getfps(), (0,0,pose_video.width,pose_video.height)) for t_id in time_ids]\n",
    "pose_grid_video_name = create_grid_video(pose_video_clip_data,video_clip_duration,speed=0.5) # duration is in seconds!!\n",
    "display.Video(pose_grid_video_name, embed=True,html_attributes=\"loop autoplay\", width=600,height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Visualize anchor vs distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Anchor together with distant embeddings\n",
    "# select other embeddings from the 80% distance percentiles\n",
    "dist_percentile = 80\n",
    "\n",
    "dist_thr = np.percentile(dist_orig, dist_percentile)\n",
    "\n",
    "time_idx_other = np.where(dist_orig > dist_thr)[0].reshape(-1)\n",
    "sampled_idx = np.random.choice(time_idx_other, 8, replace=False)\n",
    "# select anchors\n",
    "video_clip_data_distant = [(selected_video_file, t_id/ video.getfps(), (0,0,video.width,video.height)) for t_id in [anchor_idx, *sampled_idx]]\n",
    "#print(video_clip_data)\n",
    "grid_video_name_distant = create_grid_video(video_clip_data_distant,video_clip_duration,speed=0.5) # duration is in seconds!!\n",
    "\n",
    "dist_matrix = np.round(squareform(pdist(latent_vectors_all[[anchor_idx, *sampled_idx]])), 3)\n",
    "print(f\"Distances:\\n {dist_matrix}\")\n",
    "display.Video(grid_video_name_distant, embed=True,html_attributes=\"loop autoplay\", width=600,height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Compare video clips of a bunch of randomly selected anchors vs neighbors and distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pick_n_anchors = 3 # how many anchor ids to pick randomly\n",
    "\n",
    "\n",
    "random_anchor_ids = np.random.choice(np.arange(0, latent_vectors_all.shape[0]), pick_n_anchors, replace=False)\n",
    "min_frame_distance = int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"]/video.getfps()\n",
    "\n",
    "video_stack = []\n",
    "for a_idx in random_anchor_ids:\n",
    "    video_stack.append(create_visual_comparison(a_idx, latent_vectors_all, min_frame_distance, selected_video_file,video_clip_duration, upper_dist_percentile=80))\n",
    "\n",
    "# plot next to each other: left side: anchor and its 8 closest neighbors; right side anchor and 8 samples belonging to the \n",
    "# 20% of the most distant latent vectors wrt. the anchor embedding\n",
    "grid = GridspecLayout(pick_n_anchors, 2)\n",
    "# sorted video files\n",
    "for i_row, video_pair in enumerate(video_stack):\n",
    "    for j_vid,video_f in enumerate(video_pair):\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(display.Video(video_f, embed=True, html_attributes=\"loop autoplay\", width=450,height=450))\n",
    "        grid[i_row, j_vid] = out\n",
    "grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Diluting the latent space by removing samples iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frame_rate = config[\"time_window\"]\n",
    "min_remaining_dataset = 0.001 # minimum fraction of remaining samples  e.g. 0.1 = 10%\n",
    "neighbor_percentile = 1 # remove vectors which are temporally close to the sampled anchor if the belong to its closest N% percentile of embeddings in the latent space\n",
    "remaining_embeddings, remaining_time_ids = thin_dataset_iteratively(\n",
    "    latent_vectors_all, min_remaining_dataset, neighbor_percentile, min_frame_rate)\n",
    "print(f\"{len(remaining_embeddings)} remaining samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_thinned = TSNE(perplexity=30).fit_transform(remaining_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_func = umap.UMAP(densmap=True,n_components=2,min_dist=0.0001,n_neighbors=30, random_state=config[\"random_state\"])\n",
    "umap_vectors = umap_func.fit_transform(remaining_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Sample and visualize anchors and neighbors / distant sampples from the thinned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pick_n_anchors = 5 # how many anchor ids to pick randomly\n",
    "\n",
    "random_anchor_ids = np.random.choice(np.arange(0, remaining_embeddings.shape[0]), pick_n_anchors, replace=False)\n",
    "min_frame_distance = int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "\n",
    "video = KinVideo(selected_video_file, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"]/video.getfps()\n",
    "\n",
    "video_stack = []\n",
    "sampled_idx_stack = []\n",
    "for a_idx in random_anchor_ids:\n",
    "    output = create_visual_comparison(a_idx, remaining_embeddings, min_frame_distance, selected_video_file,video_clip_duration, upper_dist_percentile=80, time_idx=remaining_time_ids, return_sampled_idx=True)\n",
    "    video_close, video_distant, samples_close, samples_distant = output\n",
    "    video_stack.append((video_close, video_distant))\n",
    "    sampled_idx_stack.append((samples_close, samples_distant))\n",
    "\n",
    "# plot next to each other: left side: anchor and its 8 closest neighbors; right side anchor and 8 samples belonging to the \n",
    "# 20% of the most distant latent vectors wrt. the anchor embedding\n",
    "grid = GridspecLayout(pick_n_anchors * 2, 2)\n",
    "# sorted video files\n",
    "for i_row, video_pair in enumerate(video_stack):\n",
    "    for j_vid,video_f in enumerate(video_pair):\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(display.Video(video_f, embed=True, html_attributes=\"loop autoplay\", width=450,height=450))\n",
    "        grid[i_row * 2, j_vid] = out\n",
    "        \n",
    "        dist_matrix = np.round(squareform(pdist(latent_vectors_all[sampled_idx_stack[i_row][j_vid]])), 2)\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(display.Pretty(f\"Distances: \\n {dist_matrix}\"))\n",
    "        grid[i_row * 2+1, j_vid] = out\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Visualize the sampled anchors with their neighbors and distant samples in a t-SNE and UMAP\n",
    "\n",
    "Idea: check how close samples which are neighbors in the latent space will be projected in t-SNE / UMAP.\n",
    "To use the t-SNE / UMAP projections for clustering the anchor and its close neighbors should also be close in the t-SNE/ UMPA projection and samples which are distant to the selected anchor should also be distant to the anchor in the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the sampled points in the TSNE plot\n",
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots(len(sampled_idx_stack), 2)\n",
    "fig.set_size_inches(4 * 2, len(sampled_idx_stack) * 4)\n",
    "for i_row, (s_close, s_distant) in enumerate(sampled_idx_stack):\n",
    "    anchor_idx = s_close[0]\n",
    "    ax[i_row, 0].plot (tsne_thinned[:, 0], tsne_thinned[:, 1], 'k.', label='TSNE')\n",
    "    ax[i_row, 0].plot (tsne_thinned[np.isin(remaining_time_ids, s_close[1:]), 0], tsne_thinned[np.isin(remaining_time_ids, s_close[1:]), 1], 'go', label='Close Neighbors')\n",
    "    ax[i_row, 0].plot (tsne_thinned[np.isin(remaining_time_ids, s_distant[1:]), 0], tsne_thinned[np.isin(remaining_time_ids, s_distant[1:]), 1], 'ro', label='Distant Samples')\n",
    "    ax[i_row, 0].plot (tsne_thinned[remaining_time_ids == anchor_idx, 0], tsne_thinned[remaining_time_ids == anchor_idx, 1], 'yo', label='Anchor')\n",
    "    \n",
    "    ax[i_row, 1].plot (umap_vectors[:, 0], umap_vectors[:, 1], 'k.', label='UMAP')\n",
    "    ax[i_row, 1].plot (umap_vectors[np.isin(remaining_time_ids, s_close[1:]), 0], umap_vectors[np.isin(remaining_time_ids, s_close[1:]), 1], 'go', label='Close Neighbors')\n",
    "    ax[i_row, 1].plot (umap_vectors[np.isin(remaining_time_ids, s_distant[1:]), 0], umap_vectors[np.isin(remaining_time_ids, s_distant[1:]), 1], 'ro', label='Distant Samples')\n",
    "    ax[i_row, 1].plot (umap_vectors[remaining_time_ids == anchor_idx, 0], umap_vectors[remaining_time_ids == anchor_idx, 1], 'yo', label='Anchor')\n",
    "    \n",
    "    ax[i_row, 0].legend()\n",
    "    ax[i_row, 1].legend()\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: after running the code multiple times with different anchors, the close neighbors in the latent space tend to be also closer than the distant neighbors in the t-SNE/UMAP projection. However, quite often the anchor and its close neighbors are not concentrated at one point in the projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)  Cluster the thinned dataset\n",
    "Test different clustering approaches and visualize samples from the clusters for visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1) fuzzy c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 30\n",
    "fcm = FCM(n_clusters=n_clusters, m=1.1)\n",
    "\n",
    "fcm.fit(remaining_embeddings)\n",
    "\n",
    "# output\n",
    "fcm_centers = fcm.centers\n",
    "fcm_labels_soft = fcm.soft_predict(remaining_embeddings)\n",
    "#c_ids, label_counts = np.unique(fcm_labels, return_counts=True)\n",
    "#mapped_counts = [(c_id, c_count) for c_id, c_count in zip(c_ids, label_counts)]\n",
    "#print(f\"Counts per label: {mapped_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcm_labels_soft = fcm.soft_predict(remaining_embeddings)\n",
    "%matplotlib widget\n",
    "# plot dist showing the neighbors but not the excluded ones?\n",
    "# plotting dist removing only the nearby neighbors\n",
    "bins_clustering = 20\n",
    "hist_range_fuzzy = (0, 1)\n",
    "\n",
    "sns.histplot(data=fcm_labels_soft,binrange=(0,1), bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogramm of the largest cluster assignment score per sample\n",
    "# How many samples have one cluster assignment score of at least P?\n",
    "%matplotlib widget\n",
    "cluster_assignment_thr = 0.7\n",
    "\n",
    "fcm_max_val = np.max(fcm_labels_soft, axis=1)\n",
    "plt.hist(fcm_max_val, bins=20, range=(0,1), density=True, cumulative=-1)\n",
    "\n",
    "thr_fraction = round(sum(fcm_max_val > cluster_assignment_thr) / len(fcm_max_val) * 100, 2)\n",
    "print(f\"{thr_fraction}% of the samples have a largest cluster assignment score above {cluster_assignment_thr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_time_idx = remaining_time_ids[np.max(fcm_labels_soft, axis=1) > cluster_assignment_thr]\n",
    "fcm_labels = np.argmax(fcm_labels_soft, axis=1)[np.max(fcm_labels_soft, axis=1) > cluster_assignment_thr]\n",
    "\n",
    "# predict label from soft labels and set for samples with max value < cluster_assignment_thr to -1\n",
    "fcm_labels_all = np.ones(fcm_labels_soft.shape[0]) * -1\n",
    "fcm_labels_all[np.max(fcm_labels_soft, axis=1) > cluster_assignment_thr] = np.argmax(fcm_labels_soft, axis=1)[np.max(fcm_labels_soft, axis=1) > cluster_assignment_thr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# overlay found clusters with t-SNE / UMAP Projection\n",
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(4.5 * 2, 4.5)\n",
    "\n",
    "ax[0].plot(tsne_thinned[:, 0], tsne_thinned[:, 1], 'k.', label='TSNE')\n",
    "ax[1].plot(umap_vectors[:, 0], umap_vectors[:, 1], \"k.\", label=f'UMAP')\n",
    "cmap = cm.get_cmap(\"nipy_spectral\", n_clusters)\n",
    "for i_cluster in range(n_clusters):\n",
    "    anchor_idx = s_close[0]\n",
    "    ax[0].plot(tsne_thinned[fcm_labels_all==i_cluster, 0], tsne_thinned[fcm_labels_all==i_cluster, 1],\".\", color=cmap(i_cluster), label=f'Cluster: {i_cluster}')\n",
    "    ax[1].plot(umap_vectors[fcm_labels_all==i_cluster, 0], umap_vectors[fcm_labels_all==i_cluster, 1],\".\", color=cmap(i_cluster), label=f'Cluster: {i_cluster}')\n",
    "\n",
    "        \n",
    "#ax[0].legend()\n",
    "#ax[1].legend()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observation: Some clusters found by the fuzzy-c-means tend to also also form clusters in the t-SNE/UMAP projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a cluster, sample from it and visualize\n",
    "# select only the points with a high assignment score to sample from\n",
    "\n",
    "cluster_id = 4\n",
    "\n",
    "# all selected time points including anchor and its nearest neighbors\n",
    "time_ids_cluster = np.random.choice(potential_time_idx[fcm_labels == cluster_id], 9)\n",
    "print(time_ids_cluster)\n",
    "\n",
    "video_clip_data_cluster = [(selected_video_file, t_id/ video.getfps(), (0,0,video.width,video.height)) for t_id in time_ids_cluster]\n",
    "grid_video_cluster = create_grid_video(video_clip_data_cluster,video_clip_duration,speed=0.5) # duration is in seconds!!\n",
    "\n",
    "# print Euclidean distances between the samples from the cluster\n",
    "dist_matrix_cluster = np.round(squareform(pdist(latent_vectors_all[time_ids_cluster])), 3)\n",
    "print(f\"Distances close:\\n {dist_matrix_cluster}\")\n",
    "\n",
    "display.Video(grid_video_cluster, embed=True,html_attributes=\"loop autoplay\", width=600,height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay found clusters with t-SNE / UMAP Projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2) FLAME clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c7bf34dd8152ec82eafc374d8c9e7d2e10dcd4489766d5c4929982680d7481d"
  },
  "kernelspec": {
   "display_name": "Python [conda env:venv_VAME] *",
   "language": "python",
   "name": "conda-env-venv_VAME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
