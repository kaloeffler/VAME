{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe19a25",
   "metadata": {},
   "source": [
    "# Evaluate the latent space on useen data\n",
    "\n",
    "Use a video sequence from a different specimen the model has not been trained on to explore how well the model generalizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/home/katharina/vame_approach/VAME\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vame.analysis.kinutils import KinVideo, create_grid_video\n",
    "import os\n",
    "from datetime import datetime\n",
    "from vame.util.auxiliary import read_config\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from fcmeans import FCM\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from vame.analysis.utils import (\n",
    "    create_aligned_mouse_video,\n",
    "    create_pose_snipplet,\n",
    "    create_visual_comparison,\n",
    "    thin_dataset_iteratively,\n",
    "    find_percentile_threshold,\n",
    "    estimate_fuzzifier,\n",
    "    fukuyama_sugeno_index,\n",
    ")\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from vame.initialize_project.themis_new import get_video_metadata\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c543177",
   "metadata": {},
   "source": [
    "Load latent vectors corresponding to landmarks that where unseen during training and the latent vectors corresponding to landmarks the\n",
    "model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model was trained on video sequence 0057 of rat H01\n",
    "# now we evaluate on video sequence 0089 of rat H06\n",
    "PROJECT_PATH = \"/home/katharina/vame_approach/themis_tail_belly_align\"\n",
    "PROJECT_PATH = \"/home/katharina/vame_approach/tb_align_0089\"\n",
    "\n",
    "# path where the original videos are stored\n",
    "VIDEO_ROOT = \"/media/Themis/Data/Video\"\n",
    "\n",
    "USE_ALIGNED_VIDEO = True\n",
    "\n",
    "trained_models = [\n",
    "    (datetime.strptime(element, \"%m-%d-%Y-%H-%M\"), element)\n",
    "    for element in os.listdir(os.path.join(PROJECT_PATH, \"model\"))\n",
    "]\n",
    "# sort by time step\n",
    "trained_models.sort(key=lambda x: x[0])\n",
    "latest_model = trained_models[-1][-1]\n",
    "\n",
    "config_file = os.path.join(PROJECT_PATH, \"model\", latest_model, \"config.yaml\")\n",
    "config = read_config(config_file)\n",
    "\n",
    "\n",
    "data_file_trained_on = \"landmarks_0089_3843S2B10Gaussians_E149_confidece\"\n",
    "data_file_unseen = \"landmarks_0087_3843S2B10Gaussians_E149_confidece\"\n",
    "latent_vectors_trained_on = np.load(\n",
    "    os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        \"inference\",\n",
    "        \"results\",\n",
    "        latest_model,\n",
    "        \"latent_vectors_\" + data_file_trained_on + \".npy\",\n",
    "    )\n",
    ")\n",
    "latent_vectors_unseen = np.load(\n",
    "    os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        \"inference\",\n",
    "        \"results\",\n",
    "        latest_model,\n",
    "        \"latent_vectors_\" + data_file_unseen + \".npy\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e997f2e",
   "metadata": {},
   "source": [
    "## 1) Compare video clips of randomly sampled anchors vs neighbors and distant samples\n",
    "## from the dataset unseen during training\n",
    "\n",
    "Sample several anchors from the latent space and visualize them with their close neighbors vs distant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get video files to show video snipplets later\n",
    "video_info = get_video_metadata(VIDEO_ROOT, None)\n",
    "video_id_trained_on = data_file_trained_on.split(\"_\")[1]\n",
    "video_id_unseen = data_file_unseen.split(\"_\")[1]\n",
    "\n",
    "if USE_ALIGNED_VIDEO:\n",
    "    video_file_trained_on = os.path.join(\n",
    "        PROJECT_PATH, \"videos\", \"aligned_videos\", \"a\" + video_id_trained_on + \".MP4\",\n",
    "    )\n",
    "    video_file_unseen = os.path.join(\n",
    "        PROJECT_PATH, \"videos\", \"aligned_videos\", \"a\" + video_id_unseen + \".MP4\",\n",
    "    )\n",
    "else:\n",
    "    video_file_trained_on = os.path.join(\n",
    "        video_info[video_info[\"vid_file\"] == video_id_trained_on + \".MP4\"][\n",
    "            \"vid_folder\"\n",
    "        ].values[0],\n",
    "        video_id_trained_on + \".MP4\",\n",
    "    )\n",
    "    video_file_unseen = os.path.join(\n",
    "        video_info[video_info[\"vid_file\"] == video_id_unseen + \".MP4\"][\n",
    "            \"vid_folder\"\n",
    "        ].values[0],\n",
    "        video_id_unseen + \".MP4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_n_anchors = 3  # how many anchor ids to pick randomly\n",
    "min_dist_nn_factor = 1  # config[\"time_window\"] * min_dist_nn_factor will define the min distance in time points between the anchor and between sampled neighbors\n",
    "\n",
    "_, _, camera_pos, _ = Path(video_file_unseen).parts[-4:]\n",
    "\n",
    "\n",
    "random_anchor_ids = np.random.choice(\n",
    "    np.arange(0, latent_vectors_unseen.shape[0]), pick_n_anchors, replace=False\n",
    ")\n",
    "min_frame_distance = int(config[\"time_window\"] * min_dist_nn_factor)\n",
    "\n",
    "video = KinVideo(video_file_unseen, view=camera_pos)\n",
    "video.probevid()\n",
    "video_clip_duration = config[\"time_window\"] / video.getfps()\n",
    "\n",
    "video_stack = []\n",
    "for a_idx in random_anchor_ids:\n",
    "    video_stack.append(\n",
    "        create_visual_comparison(\n",
    "            a_idx,\n",
    "            latent_vectors_unseen,\n",
    "            min_frame_distance,\n",
    "            video_file_unseen,\n",
    "            video_clip_duration,\n",
    "            upper_dist_percentile=80,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# plot next to each other: left side: anchor and its 8 closest neighbors; right side anchor and 8 samples belonging to the\n",
    "# 20% of the most distant latent vectors wrt. the anchor embedding\n",
    "grid = GridspecLayout(pick_n_anchors, 2)\n",
    "# sorted video files\n",
    "for i_row, video_pair in enumerate(video_stack):\n",
    "    for j_vid, video_f in enumerate(video_pair):\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(\n",
    "                display.Video(\n",
    "                    video_f,\n",
    "                    embed=True,\n",
    "                    html_attributes=\"loop autoplay\",\n",
    "                    width=450,\n",
    "                    height=450,\n",
    "                )\n",
    "            )\n",
    "        grid[i_row, j_vid] = out\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ebf62",
   "metadata": {},
   "source": [
    "## 2) Dilution of latent vectors\n",
    "\n",
    "Dilute the latent vectors since they correspond to heavily overlapping time series of the landmarks, hence to latent vectors can essentially correspond to the same input time series just shifted by a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove vectors which are temporally close to the sampled anchor if the belong to its closest N% percentile of embeddings in the latent space\n",
    "min_dist_in_frames = config[\"time_window\"]\n",
    "sub_sampling_factor = (\n",
    "    config[\"time_window\"] // 10\n",
    ")  # choose a subsampling factor for neighbor percentile estimation to save memory\n",
    "neighbor_percentile_unseen = find_percentile_threshold(\n",
    "    latent_vectors_unseen[::sub_sampling_factor],\n",
    "    config[\"time_window\"],\n",
    "    time_idx=np.arange(0, len(latent_vectors_unseen))[::sub_sampling_factor],\n",
    "    test_fraction=0.01 * sub_sampling_factor,\n",
    ")\n",
    "print(f\"Selected neigbor percentile unseen dataset: {neighbor_percentile_unseen}\")\n",
    "\n",
    "min_remaining_dataset = 0.001  # minimum fraction of remaining samples  e.g. 0.1 = 10%\n",
    "remaining_embeddings_unseen, remaining_time_ids_unseen = thin_dataset_iteratively(\n",
    "    latent_vectors_unseen,\n",
    "    min_remaining_dataset,\n",
    "    neighbor_percentile_unseen,\n",
    "    min_dist_in_frames,\n",
    ")\n",
    "percentile_data_unseen = np.round(\n",
    "    len(remaining_embeddings_unseen) / len(latent_vectors_unseen) * 100, 2\n",
    ")\n",
    "print(\n",
    "    f\"{len(remaining_embeddings_unseen)} remaining samples from orignially {len(latent_vectors_unseen)}. So just {percentile_data_unseen}% of the original dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_percentile_trained_on = find_percentile_threshold(\n",
    "    latent_vectors_trained_on[::sub_sampling_factor],\n",
    "    config[\"time_window\"],\n",
    "    time_idx=np.arange(0, len(latent_vectors_trained_on))[::sub_sampling_factor],\n",
    "    test_fraction=0.01 * sub_sampling_factor,\n",
    ")\n",
    "print(\n",
    "    f\"Selected neigbor percentile trained on dataset: {neighbor_percentile_trained_on}\"\n",
    ")\n",
    "\n",
    "(\n",
    "    remaining_embeddings_trained_on,\n",
    "    remaining_time_ids_trained_on,\n",
    ") = thin_dataset_iteratively(\n",
    "    latent_vectors_trained_on,\n",
    "    min_remaining_dataset,\n",
    "    neighbor_percentile_trained_on,\n",
    "    min_dist_in_frames,\n",
    ")\n",
    "percentile_data_trained_on = np.round(\n",
    "    len(remaining_embeddings_trained_on) / len(latent_vectors_trained_on) * 100, 2\n",
    ")\n",
    "print(\n",
    "    f\"{len(remaining_embeddings_trained_on)} remaining samples from orignially {len(latent_vectors_trained_on)}. So just {percentile_data_trained_on}% of the original dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a141b",
   "metadata": {},
   "source": [
    "## 3) Clustering the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a1cbb",
   "metadata": {},
   "source": [
    "### 3.1) Estimate the fuzzifier and the number of clusters\n",
    "Use the diluted latent vectors corresponding to the time series the model was trained on and estimate the fuzzifier and the number of clusters for the fuzzy-c-means clustering method. Use the Fukuyama Sugeno index to find the number of needed clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1356b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of clusters\n",
    "test_n_clusters = np.arange(1, 20)\n",
    "N_samples, M_feat = remaining_embeddings_trained_on.shape\n",
    "m = estimate_fuzzifier(N_samples, M_feat)\n",
    "print(m)\n",
    "fcm_models = [FCM(n_clusters=i_clusters, m=m) for i_clusters in test_n_clusters]\n",
    "for i_model in range(len(fcm_models)):\n",
    "    if i_model % 5 == 0:\n",
    "        print(f\"fitting model: {i_model}/{len(test_n_clusters)}\")\n",
    "    fcm_models[i_model].fit(remaining_embeddings_trained_on)\n",
    "\n",
    "all_labels = [\n",
    "    fcm_models[i_mode].soft_predict(remaining_embeddings_trained_on)\n",
    "    for i_mode, i_clusters in enumerate(test_n_clusters)\n",
    "]\n",
    "fs_scores = [\n",
    "    fukuyama_sugeno_index(\n",
    "        remaining_embeddings_trained_on, labels, fcm_models[i].centers, m\n",
    "    )\n",
    "    for i, labels in enumerate(all_labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bdeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.plot(fs_scores, \".\")\n",
    "plt.title(\"Fukuyama Sugeno Index\")\n",
    "plt.xlabel(\"N Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef18f6",
   "metadata": {},
   "source": [
    "### 3.2) Train a UMAP projection and show the visualize the learned clusters\n",
    "Train a UMAP projection on the diluted latent vectors corresponding to the time series the model has been trained on and use this UMAP projection for the diluted latent vectors corresponding to the unseen time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e290d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_trained_on = umap.UMAP(\n",
    "    n_components=2, min_dist=0.0001, n_neighbors=30, random_state=config[\"random_state\"]\n",
    ").fit(remaining_embeddings_trained_on)\n",
    "\n",
    "umap_embedding_trained_on = umap_trained_on.transform(remaining_embeddings_trained_on)\n",
    "umap_embedding_unseen = umap_trained_on.transform(remaining_embeddings_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_fcm = 16\n",
    "idx = np.where(n_clusters_fcm == test_n_clusters)[0][0]\n",
    "fcm = fcm_models[idx]\n",
    "\n",
    "# output\n",
    "# output is [N,K]: N number of latent embeddings and K the number of clusters; for where each entry is a membership score between 0...1\n",
    "fcm_labels_soft_trained_on = fcm.soft_predict(remaining_embeddings_trained_on)\n",
    "fcm_labels_soft_unseen = fcm.soft_predict(remaining_embeddings_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c3499",
   "metadata": {},
   "source": [
    "Explore the fuzzy clusters that where found in the high dim latent space by plotting for each sample its membership encoded as alpha (transparency) in the UMAP projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611dc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "n_cols = int(n_clusters_fcm ** 0.5)\n",
    "n_rows = int(np.ceil(n_clusters_fcm / n_cols))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(9, (9 / n_cols) * n_rows)\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_fcm)\n",
    "for i_cluster in range(n_clusters_fcm):\n",
    "    i_col = i_cluster % n_cols\n",
    "    i_row = i_cluster // n_cols\n",
    "    ax[i_row][i_col].scatter(\n",
    "        umap_embedding_unseen[:, 0],\n",
    "        umap_embedding_unseen[:, 1],\n",
    "        color=\"w\",\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "    ax[i_row][i_col].scatter(\n",
    "        umap_embedding_unseen[:, 0],\n",
    "        umap_embedding_unseen[:, 1],\n",
    "        color=cmap(i_cluster),\n",
    "        alpha=list(fcm_labels_soft_unseen[:, i_cluster]),\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff54bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign each latent vector to the cluster with the highest membership score\n",
    "cluster_membership_thr = 0.6\n",
    "fcm_labels_unseen = np.ones(fcm_labels_soft_unseen.shape[0]) * -1\n",
    "fcm_labels_unseen[\n",
    "    np.max(fcm_labels_soft_unseen, axis=1) > cluster_membership_thr\n",
    "] = np.argmax(fcm_labels_soft_unseen, axis=1)[\n",
    "    np.max(fcm_labels_soft_unseen, axis=1) > cluster_membership_thr\n",
    "]\n",
    "\n",
    "fcm_labels_trained_on = np.ones(fcm_labels_soft_trained_on.shape[0]) * -1\n",
    "fcm_labels_trained_on[\n",
    "    np.max(fcm_labels_soft_trained_on, axis=1) > cluster_membership_thr\n",
    "] = np.argmax(fcm_labels_soft_trained_on, axis=1)[\n",
    "    np.max(fcm_labels_soft_trained_on, axis=1) > cluster_membership_thr\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea34bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 14\n",
    "\n",
    "\n",
    "# all selected time points including anchor and its nearest neighbors\n",
    "time_ids_cluster_unseen = np.random.choice(\n",
    "    remaining_time_ids_unseen[fcm_labels_unseen == cluster_id],\n",
    "    min(16, np.sum(fcm_labels_unseen == cluster_id)),\n",
    "    replace=False,\n",
    ")\n",
    "\n",
    "video_clip_data_cluster_unseen = [\n",
    "    (video_file_unseen, t_id / video.getfps(), (0, 0, video.width, video.height))\n",
    "    for t_id in time_ids_cluster_unseen\n",
    "]\n",
    "grid_video_cluster = create_grid_video(\n",
    "    video_clip_data_cluster_unseen, video_clip_duration, speed=0.5, nrows=4, ncols=4\n",
    ")  # duration is in seconds!!\n",
    "display.Video(\n",
    "    grid_video_cluster,\n",
    "    embed=True,\n",
    "    html_attributes=\"loop autoplay\",\n",
    "    width=600,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster the data not trained on using the clusters found by the data the model was trained on. Then compare samples referring\n",
    "## to the same cluster that origin from the video data unseen to the model vs the video data the model was trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da86022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples for each cluster left: samples from the video sequence not used in training; right samples from the video sequence used in training\n",
    "\n",
    "grid = GridspecLayout(n_clusters_fcm, 2)\n",
    "# sorted video files\n",
    "\n",
    "for i_cluster_id in range(n_clusters_fcm):\n",
    "    # sample video clips corresponding to a single cluster of the data NOT used in training\n",
    "    time_ids_cluster_unseen = np.random.choice(\n",
    "        remaining_time_ids_unseen[fcm_labels_unseen == i_cluster_id],\n",
    "        min(16, np.sum(fcm_labels_unseen == i_cluster_id)),\n",
    "        replace=False,\n",
    "    )\n",
    "    video_clip_data_cluster_unseen = [\n",
    "        (video_file_unseen, t_id / video.getfps(), (0, 0, video.width, video.height))\n",
    "        for t_id in time_ids_cluster_unseen\n",
    "    ]\n",
    "    if len(time_ids_cluster_unseen) > 0:\n",
    "        grid_video_cluster_unseen = create_grid_video(\n",
    "            video_clip_data_cluster_unseen,\n",
    "            video_clip_duration,\n",
    "            speed=0.5,\n",
    "            nrows=4,\n",
    "            ncols=4,\n",
    "        )\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(\n",
    "                display.Video(\n",
    "                    grid_video_cluster_unseen,\n",
    "                    embed=True,\n",
    "                    html_attributes=\"loop autoplay\",\n",
    "                    width=450,\n",
    "                    height=450,\n",
    "                )\n",
    "            )\n",
    "        grid[i_cluster_id, 0] = out\n",
    "\n",
    "    # sample video clips corresponding to a single cluster of the data used in training\n",
    "    time_ids_cluster_trained_on = np.random.choice(\n",
    "        remaining_time_ids_trained_on[fcm_labels_trained_on == i_cluster_id],\n",
    "        min(16, np.sum(fcm_labels_trained_on == i_cluster_id)),\n",
    "        replace=False,\n",
    "    )\n",
    "    if len(time_ids_cluster_trained_on) > 0:\n",
    "        video_clip_data_cluster_trained_on = [\n",
    "            (\n",
    "                video_file_trained_on,\n",
    "                t_id / video.getfps(),\n",
    "                (0, 0, video.width, video.height),\n",
    "            )\n",
    "            for t_id in time_ids_cluster_trained_on\n",
    "        ]\n",
    "        grid_video_cluster_trained_on = create_grid_video(\n",
    "            video_clip_data_cluster_trained_on,\n",
    "            video_clip_duration,\n",
    "            speed=0.5,\n",
    "            nrows=4,\n",
    "            ncols=4,\n",
    "        )\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(\n",
    "                display.Video(\n",
    "                    grid_video_cluster_trained_on,\n",
    "                    embed=True,\n",
    "                    html_attributes=\"loop autoplay\",\n",
    "                    width=450,\n",
    "                    height=450,\n",
    "                )\n",
    "            )\n",
    "        grid[i_cluster_id, 1] = out\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb283012",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5) Compare the behavior over time? - cluster the full latent space and then see how the cluster labels progress over time\n",
    "clusters_all_trained_on = fcm.soft_predict(latent_vectors_trained_on)\n",
    "clusters_all_unseen = fcm.soft_predict(latent_vectors_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "cmap = cm.get_cmap(\"rainbow\", n_clusters_fcm)\n",
    "\n",
    "for i_cluster in range(n_clusters_fcm):\n",
    "    ax[0].plot(clusters_all_trained_on[:, i_cluster], c=cmap(i_cluster))\n",
    "    ax[1].plot(clusters_all_unseen[:, i_cluster], c=cmap(i_cluster))\n",
    "ax[0].set_title(\"data trained on\")\n",
    "ax[1].set_title(\"data unseen during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321dcb6",
   "metadata": {},
   "source": [
    "## 4) Reduce the latent space using a PCA and then apply the clustering again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "pca = PCA(n_components=11)\n",
    "pca_vectors_trained_on = pca.fit_transform(remaining_embeddings_trained_on)\n",
    "pca_vectors_unseen = pca.transform(remaining_embeddings_unseen)\n",
    "dbscan = DBSCAN(eps=2.2)\n",
    "\n",
    "dbscan_labels_trained_on = dbscan.fit_predict(pca_vectors_trained_on)\n",
    "# dbscan_labels_unseen = dbscan.predict(pca_vectors_unseen)\n",
    "n_dbscan_labels = int(np.max(dbscan_labels_trained_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(dbscan_labels_trained_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74792fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridspecLayout(n_dbscan_labels, 2)\n",
    "# sorted video files\n",
    "\n",
    "for i_cluster_id in range(n_dbscan_labels):\n",
    "    # sample video clips corresponding to a single cluster of the data NOT used in training\n",
    "    # time_ids_cluster_unseen = np.random.choice(remaining_time_ids_unseen[fcm_labels_unseen == i_cluster_id], min(16, np.sum(fcm_labels_unseen == i_cluster_id)), replace=False)\n",
    "    # video_clip_data_cluster_unseen = [(video_file_unseen, t_id/ video.getfps(), (0,0,video.width,video.height)) for t_id in time_ids_cluster_unseen]\n",
    "    # if len(time_ids_cluster_unseen) > 0:\n",
    "    #    grid_video_cluster_unseen = create_grid_video(video_clip_data_cluster_unseen,video_clip_duration,speed=0.5,nrows=4,ncols=4)\n",
    "    #    out = Output()\n",
    "    #    with out:\n",
    "    #        display.display(display.Video(grid_video_cluster_unseen, embed=True, html_attributes=\"loop autoplay\", width=450,height=450))\n",
    "    #    grid[i_cluster_id, 0] = out\n",
    "\n",
    "    # sample video clips corresponding to a single cluster of the data used in training\n",
    "    time_ids_cluster_trained_on = np.random.choice(\n",
    "        remaining_time_ids_trained_on[dbscan_labels_trained_on == i_cluster_id],\n",
    "        min(16, np.sum(dbscan_labels_trained_on == i_cluster_id)),\n",
    "        replace=False,\n",
    "    )\n",
    "    if len(time_ids_cluster_trained_on) > 0:\n",
    "        video_clip_data_cluster_trained_on = [\n",
    "            (\n",
    "                video_file_trained_on,\n",
    "                t_id / video.getfps(),\n",
    "                (0, 0, video.width, video.height),\n",
    "            )\n",
    "            for t_id in time_ids_cluster_trained_on\n",
    "        ]\n",
    "        grid_video_cluster_trained_on = create_grid_video(\n",
    "            video_clip_data_cluster_trained_on,\n",
    "            video_clip_duration,\n",
    "            speed=0.5,\n",
    "            nrows=4,\n",
    "            ncols=4,\n",
    "        )\n",
    "        out = Output()\n",
    "        with out:\n",
    "            display.display(\n",
    "                display.Video(\n",
    "                    grid_video_cluster_trained_on,\n",
    "                    embed=True,\n",
    "                    html_attributes=\"loop autoplay\",\n",
    "                    width=450,\n",
    "                    height=450,\n",
    "                )\n",
    "            )\n",
    "        grid[i_cluster_id, 1] = out\n",
    "        \n",
    "# just h\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dim_red = PCA(n_components=20)\n",
    "pca_dim_red.fit(remaining_embeddings_unseen)\n",
    "print(\n",
    "    f\"Explained variance cumulated over the dimensions: {pca_dim_red.explained_variance_ratio_.cumsum()}\"\n",
    ")\n",
    "\n",
    "pca_dim_red = PCA(n_components=20)\n",
    "pca_dim_red.fit(remaining_embeddings_trained_on)\n",
    "print(\n",
    "    f\"Explained variance cumulated over the dimensions: {pca_dim_red.explained_variance_ratio_.cumsum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ab244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv_VAME] *",
   "language": "python",
   "name": "conda-env-venv_VAME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
