{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9865ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate the triplets from Shuki available at- https://github.com/ysterin/deep_cluster/tree/master/deep_cluster/triplets/data\n",
    "\n",
    "According to Shuki: selected_triplets.csv contains the most data\n",
    "the files:\n",
    "selected_triplets_strong.csv\n",
    "selected_triplets_robust.csv\n",
    "both contain less data but clearer classifications\n",
    "\n",
    "selected_tiplets_fixed.csv should have a repair for reading the data, claims there was a redundant comma or some other small mistake\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from vame.util.auxiliary import read_config\n",
    "import ast\n",
    "from clean_up_triplets import clean_raw_triplets_file\n",
    "from vame.analysis.kinutils import KinVideo, create_grid_video\n",
    "from IPython import display\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "\n",
    "# dowloaded triplets from github and saved locally\n",
    "# CAUTION: some of the csv files contain the file header (video_file, anchor, sample1,sample2, selected) several times within the csv file! -> additional\n",
    "# data cleaning needed!\n",
    "TRIPLETS_PATH = \"/home/katharina/vame_approach/triplets_shuki\"\n",
    "\n",
    "# project path with already predicted latent vectors -> run the themis_pipeline.py script first\n",
    "# also create aligned videos to visualize the triplets by running create_aligned_videos.py!\n",
    "PROJECT_PATH = \"/home/katharina/vame_approach/tb_align_0089\"\n",
    "LATENT_VEC_PATH = os.path.join(PROJECT_PATH, \"inference\", \"results\", \"06-08-2022-09-36\")\n",
    "ALIGNED_VIDEO_PATH = \"/home/katharina/vame_approach/tb_align_0089/videos/aligned_videos\"\n",
    "\n",
    "config = read_config(\n",
    "    os.path.join(\n",
    "        PROJECT_PATH, \"model\", os.path.basename(LATENT_VEC_PATH), \"config.yaml\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = clean_raw_triplets_file(\n",
    "    os.path.join(TRIPLETS_PATH, \"selected_triplets_fixed.csv\")\n",
    ")\n",
    "# triplets = clean_raw_triplets_file(os.path.join(TRIPLETS_PATH, \"strong_triplets.csv\"))\n",
    "\n",
    "\n",
    "triplets[\"video_id\"] = triplets[\"video_file\"].apply(\n",
    "    lambda x: os.path.basename(x).split(\".\")[0]\n",
    ")\n",
    "triplets[\"anchor\"] = triplets[\"anchor\"].apply(lambda x: ast.literal_eval(x))\n",
    "triplets[\"sample1\"] = triplets[\"sample1\"].apply(lambda x: ast.literal_eval(x))\n",
    "triplets[\"sample2\"] = triplets[\"sample2\"].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "video_ids = np.unique(triplets[\"video_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector_files = {\n",
    "    re.findall(r\"\\d+\", file)[0]: os.path.join(LATENT_VEC_PATH, file)\n",
    "    for file in os.listdir(LATENT_VEC_PATH)\n",
    "    if re.findall(r\"\\d+\", file)[0] in video_ids\n",
    "}\n",
    "latent_vectors = {\n",
    "    video_id: np.load(file) for video_id, file in latent_vector_files.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_video_files = {\n",
    "    v_id: os.path.join(ALIGNED_VIDEO_PATH, \"a\" + v_id + \".MP4\")\n",
    "    for v_id in video_ids\n",
    "    if os.path.exists(os.path.join(ALIGNED_VIDEO_PATH, \"a\" + v_id + \".MP4\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = KinVideo(aligned_video_files[list(aligned_video_files.keys())[0]], view=\"\")\n",
    "video.probevid()\n",
    "video_clip_duration = (\n",
    "    triplets.iloc[0][\"anchor\"][1] - triplets.iloc[0][\"anchor\"][0]\n",
    ") / video.getfps()  # lenght of the triplet snipplet in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all triplets which have no corresponding aligned video file (and no latent vectors)\n",
    "triplets = triplets[\n",
    "    np.isin(triplets[\"video_id\"].values, list(aligned_video_files.keys()))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8b429",
   "metadata": {},
   "source": [
    "### The \"selected\" field in the triplets has values 0, 1 or 2. 0: impossible to decide which of the two samples is closer to the anchor. 1: sample1 is closer to the anchor than sample2. 2: sample2 is closer to the anchor than sample1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fbd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all triplets labelled with 0 in selected field\n",
    "triplets = triplets[triplets[\"selected\"] != 0]\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ece27",
   "metadata": {},
   "source": [
    "## Visualize the Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = np.random.choice(len(triplets), min(15, len(triplets)), replace=False)\n",
    "grid = GridspecLayout(len(selected_ids), 1)\n",
    "for i_row, s_id in enumerate(selected_ids):\n",
    "    video_clip_data = [\n",
    "        (\n",
    "            aligned_video_files[triplets.iloc[s_id][\"video_id\"]],\n",
    "            triplets.iloc[s_id][field][0] / video.getfps(),\n",
    "            (0, 0, video.width, video.height),\n",
    "        )\n",
    "        for field in [\"anchor\", \"sample1\", \"sample2\"]\n",
    "    ]\n",
    "    grid_video_name = create_grid_video(\n",
    "        video_clip_data, video_clip_duration, speed=0.5\n",
    "    )  # duration is in seconds!!\n",
    "    out = Output()\n",
    "    print(\"Selected id: \", triplets.iloc[s_id][\"selected\"])\n",
    "    with out:\n",
    "        display.display(\n",
    "            display.Video(\n",
    "                grid_video_name,\n",
    "                embed=True,\n",
    "                html_attributes=\"loop autoplay\",\n",
    "                width=600,\n",
    "                height=200,\n",
    "            )\n",
    "        )\n",
    "    grid[i_row, 0] = out\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e35a",
   "metadata": {},
   "source": [
    "## Calculate distances latent vectors \n",
    "Check whether the distances between anchor and close sample are smaller than between anchor and distant sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the triplets and the temp. window used to create the latent vectors are of different sizes\n",
    "# (triplets 60 or 120 frames,time window to create latent vectors: 144)\n",
    "# therefore assume that the additional frames in the end of the time window will have a small effect\n",
    "start_idx_anchor = np.vstack(triplets[\"anchor\"].values)[:, 0]\n",
    "start_idx_sample1 = np.vstack(triplets[\"sample1\"].values)[:, 0]\n",
    "start_idx_sample2 = np.vstack(triplets[\"sample2\"].values)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c47d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_anchor = np.array(\n",
    "    [\n",
    "        latent_vectors[video_id][time_idx]\n",
    "        for video_id, time_idx in zip(triplets[\"video_id\"].values, start_idx_anchor)\n",
    "    ]\n",
    ")\n",
    "embeddings_sample1 = np.array(\n",
    "    [\n",
    "        latent_vectors[video_id][time_idx]\n",
    "        for video_id, time_idx in zip(triplets[\"video_id\"].values, start_idx_sample1)\n",
    "    ]\n",
    ")\n",
    "embeddings_sample2 = np.array(\n",
    "    [\n",
    "        latent_vectors[video_id][time_idx]\n",
    "        for video_id, time_idx in zip(triplets[\"video_id\"].values, start_idx_sample2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_anchor_s1 = np.linalg.norm(embeddings_anchor - embeddings_sample1, axis=1)\n",
    "dist_anchor_s2 = np.linalg.norm(embeddings_anchor - embeddings_sample2, axis=1)\n",
    "dist_s1_s2 = np.linalg.norm(embeddings_sample1 - embeddings_sample2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95760199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of correctly aranged distances\n",
    "correctly_arranged = np.sum(\n",
    "    (dist_anchor_s1 < dist_anchor_s2) & (triplets[\"selected\"].values == \"1\")\n",
    ") + np.sum((dist_anchor_s1 > dist_anchor_s2) & (triplets[\"selected\"].values == \"2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0038f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridspecLayout(len(triplets), 1)\n",
    "\n",
    "for i_row in range(len(triplets)):\n",
    "    video_clip_data = [\n",
    "        (\n",
    "            aligned_video_files[triplets.iloc[i_row][\"video_id\"]],\n",
    "            triplets.iloc[i_row][field][0] / video.getfps(),\n",
    "            (0, 0, video.width, video.height),\n",
    "        )\n",
    "        for field in [\"anchor\", \"sample1\", \"sample2\"]\n",
    "    ]\n",
    "    # print(video_clip_data)\n",
    "    grid_video_name = create_grid_video(\n",
    "        video_clip_data, video_clip_duration, speed=0.5\n",
    "    )  # duration is in seconds!!\n",
    "    out = Output()\n",
    "    print(\"Selected id\", triplets.iloc[i_row][\"selected\"])\n",
    "    print(\"distance anchor-sample1\", dist_anchor_s1[i_row])\n",
    "    print(\"distance anchor-sample2\", dist_anchor_s2[i_row])\n",
    "    print(\"distance sample1-sample2\", dist_s1_s2[i_row])\n",
    "\n",
    "    with out:\n",
    "        display.display(\n",
    "            display.Video(\n",
    "                grid_video_name,\n",
    "                embed=True,\n",
    "                html_attributes=\"loop autoplay\",\n",
    "                width=600,\n",
    "                height=200,\n",
    "            )\n",
    "        )\n",
    "    grid[i_row, 0] = out\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd8f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c6994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv_VAME] *",
   "language": "python",
   "name": "conda-env-venv_VAME-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
